{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn xgboost lightgbm catboost matplotlib seaborn openpyxl joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvwrdVt-EU4N",
        "outputId": "d4b09fa4-fbf8-405b-df46-3c92e6f58e0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.28.9)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##English"
      ],
      "metadata": {
        "id": "VbaSiMQNbyi5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW9_mbdTEHgs",
        "outputId": "027878fc-b372-48e5-fa26-b39ce958dc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\n",
            "================================================================================\n",
            "Class Labels: AI = 0, Human = 1\n",
            "Test Size: 0.2, CV Folds: 5, Random Seed: 42\n",
            "================================================================================\n",
            "\n",
            "Loading feature recommendations from: best_features_summary.xlsx\n",
            "Total features in file: 21\n",
            "Highly recommended features: 16\n",
            "\n",
            "Highly Recommended Features and Their Sources:\n",
            "  - transitivity: nltk\n",
            "  - top_eigenvalue: nltk_lemma\n",
            "  - avg_closeness_centrality: nltk\n",
            "  - avg_degree_centrality: nltk\n",
            "  - density: nltk\n",
            "  - algebraic_connectivity: nltk_lemma\n",
            "  - matching_number: nltk\n",
            "  - size_largest_component: nltk\n",
            "  - number_of_nodes: nltk\n",
            "  - graph_energy: nltk\n",
            "  - wiener_index: nltk\n",
            "  - avg_shortest_path_length: spacy\n",
            "  - radius: spacy\n",
            "  - diameter: simple\n",
            "  - second_eigenvalue: nltk\n",
            "  - avg_betweenness_centrality: spacy\n",
            "\n",
            "================================================================================\n",
            "BUILDING MIXED-SOURCE DATASET\n",
            "================================================================================\n",
            "\n",
            "Processing feature: transitivity from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: top_eigenvalue from source: nltk_lemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_closeness_centrality from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_degree_centrality from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: density from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: algebraic_connectivity from source: nltk_lemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: matching_number from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: size_largest_component from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: number_of_nodes from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: graph_energy from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: wiener_index from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_shortest_path_length from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: radius from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: diameter from source: simple\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: second_eigenvalue from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_betweenness_centrality from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Combining features...\n",
            "\n",
            "âœ“ Dataset built successfully!\n",
            "  Shape: (600, 17)\n",
            "  Features: 16\n",
            "  Samples: 600\n",
            "  AI samples: 300\n",
            "  Human samples: 300\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING DATA\n",
            "================================================================================\n",
            "Initial shape: (600, 16)\n",
            "\n",
            "Handling missing values (-1)...\n",
            "\n",
            "âœ“ Preprocessing complete!\n",
            "  Final shape: (600, 16)\n",
            "  Class distribution:\n",
            "    AI (0): 300\n",
            "    Human (1): 300\n",
            "\n",
            "================================================================================\n",
            "SPLITTING AND SCALING DATA\n",
            "================================================================================\n",
            "Train set: 480 samples\n",
            "Test set: 120 samples\n",
            "âœ“ Scaling complete (StandardScaler)\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Logistic Regression\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9792 (+/- 0.0066)\n",
            "Training Time: 0.019 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Ridge Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.9771 (+/- 0.0078)\n",
            "Training Time: 0.014 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SGD Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9583\n",
            "Precision: 0.9231\n",
            "Recall: 1.0000\n",
            "F1-Score: 0.9600\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.9583 (+/- 0.0219)\n",
            "Training Time: 0.003 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Decision Tree\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9583\n",
            "Precision: 0.9661\n",
            "Recall: 0.9500\n",
            "F1-Score: 0.9580\n",
            "ROC-AUC: 0.9583\n",
            "CV Accuracy: 0.9521 (+/- 0.0224)\n",
            "Training Time: 0.007 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Random Forest\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 1.0000\n",
            "Recall: 0.9500\n",
            "F1-Score: 0.9744\n",
            "ROC-AUC: 0.9987\n",
            "CV Accuracy: 0.9667 (+/- 0.0167)\n",
            "Training Time: 0.225 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Extra Trees\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9667\n",
            "Precision: 1.0000\n",
            "Recall: 0.9333\n",
            "F1-Score: 0.9655\n",
            "ROC-AUC: 0.9978\n",
            "CV Accuracy: 0.9542 (+/- 0.0276)\n",
            "Training Time: 0.124 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gradient Boosting\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9833\n",
            "Precision: 1.0000\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9831\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9667 (+/- 0.0191)\n",
            "Training Time: 0.447 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: KNN\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n",
            "ROC-AUC: 1.0000\n",
            "CV Accuracy: 0.9771 (+/- 0.0138)\n",
            "Training Time: 0.002 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (Linear)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9812 (+/- 0.0138)\n",
            "Training Time: 0.009 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9833\n",
            "Precision: 0.9833\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9833\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9729 (+/- 0.0106)\n",
            "Training Time: 0.011 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gaussian NB\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9417\n",
            "Precision: 0.9818\n",
            "Recall: 0.9000\n",
            "F1-Score: 0.9391\n",
            "ROC-AUC: 0.9844\n",
            "CV Accuracy: 0.9250 (+/- 0.0369)\n",
            "Training Time: 0.002 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9917\n",
            "Precision: 1.0000\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9916\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9688 (+/- 0.0174)\n",
            "Training Time: 0.09 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: LightGBM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9831\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9748\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9750 (+/- 0.0141)\n",
            "Training Time: 0.121 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: CatBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9831\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9748\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9708 (+/- 0.0167)\n",
            "Training Time: 5.213 seconds\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING COMPLETE - RESULTS SUMMARY\n",
            "================================================================================\n",
            "              Model  Train Time (s)  Accuracy  Precision   Recall  F1-Score  ROC-AUC  CV Mean   CV Std\n",
            "                KNN           0.002  1.000000   1.000000 1.000000  1.000000 1.000000 0.977083 0.013819\n",
            "            XGBoost           0.090  0.991667   1.000000 0.983333  0.991597 0.999444 0.968750 0.017430\n",
            "          SVM (RBF)           0.011  0.983333   0.983333 0.983333  0.983333 0.999444 0.972917 0.010623\n",
            "  Gradient Boosting           0.447  0.983333   1.000000 0.966667  0.983051 0.999444 0.966667 0.019094\n",
            "Logistic Regression           0.019  0.975000   0.967213 0.983333  0.975207 0.999167 0.979167 0.006588\n",
            "   Ridge Classifier           0.014  0.975000   0.967213 0.983333  0.975207      NaN 0.977083 0.007795\n",
            "       SVM (Linear)           0.009  0.975000   0.967213 0.983333  0.975207 0.999167 0.981250 0.013819\n",
            "           LightGBM           0.121  0.975000   0.983051 0.966667  0.974790 0.999444 0.975000 0.014130\n",
            "           CatBoost           5.213  0.975000   0.983051 0.966667  0.974790 0.999167 0.970833 0.016667\n",
            "      Random Forest           0.225  0.975000   1.000000 0.950000  0.974359 0.998750 0.966667 0.016667\n",
            "        Extra Trees           0.124  0.966667   1.000000 0.933333  0.965517 0.997778 0.954167 0.027639\n",
            "     SGD Classifier           0.003  0.958333   0.923077 1.000000  0.960000      NaN 0.958333 0.021850\n",
            "      Decision Tree           0.007  0.958333   0.966102 0.950000  0.957983 0.958333 0.952083 0.022438\n",
            "        Gaussian NB           0.002  0.941667   0.981818 0.900000  0.939130 0.984444 0.925000 0.036917\n",
            "\n",
            "âœ“ Results saved to: ml_classification_results/model_comparison_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "================================================================================\n",
            "âœ“ Comparison plot saved to: ml_classification_results/model_comparison.png\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_all.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_all.png\n",
            "\n",
            "================================================================================\n",
            "TOP 3 MODELS FOR HYPERPARAMETER TUNING:\n",
            "  1. KNN\n",
            "  2. XGBoost\n",
            "  3. SVM (RBF)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER TUNING - TOP 3 MODELS\n",
            "================================================================================\n",
            "\n",
            "No parameter grid defined for KNN, skipping tuning\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.3}\n",
            "Best CV F1-Score: 0.9750\n",
            "Tuning time: 12.49 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Best parameters: {'gamma': 0.01, 'C': 10}\n",
            "Best CV F1-Score: 0.9771\n",
            "Tuning time: 1.24 seconds\n",
            "\n",
            "âœ“ Tuning results saved to: ml_classification_results/top_models_tuning_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "FINAL EVALUATION OF TUNED MODELS\n",
            "================================================================================\n",
            "\n",
            "Evaluating: KNN\n",
            "  Accuracy: 1.0000\n",
            "  F1-Score: 1.0000\n",
            "  ROC-AUC: 1.0000\n",
            "\n",
            "Evaluating: XGBoost\n",
            "  Accuracy: 0.9667\n",
            "  F1-Score: 0.9661\n",
            "  ROC-AUC: 0.9992\n",
            "\n",
            "Evaluating: SVM (RBF)\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9752\n",
            "  ROC-AUC: 0.9994\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "    Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC\n",
            "      KNN  1.000000   1.000000 1.000000  1.000000 1.000000\n",
            "SVM (RBF)  0.975000   0.967213 0.983333  0.975207 0.999444\n",
            "  XGBoost  0.966667   0.982759 0.950000  0.966102 0.999167\n",
            "\n",
            "âœ“ Final results saved to: ml_classification_results/final_tuned_models_results.xlsx\n",
            "\n",
            "ðŸ† BEST MODEL: KNN\n",
            "   F1-Score: 1.0000\n",
            "   Accuracy: 1.0000\n",
            "\n",
            "âœ“ Best model saved to: ml_classification_results/best_model_KNN.pkl\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_tuned.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_tuned.png\n",
            "Feature importance not available for KNN\n",
            "\n",
            "================================================================================\n",
            "DETAILED CLASSIFICATION REPORT - KNN\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      1.00      1.00        60\n",
            "       Human       1.00      1.00      1.00        60\n",
            "\n",
            "    accuracy                           1.00       120\n",
            "   macro avg       1.00      1.00      1.00       120\n",
            "weighted avg       1.00      1.00      1.00       120\n",
            "\n",
            "\n",
            "âœ“ Classification report saved to: ml_classification_results/classification_report_KNN.xlsx\n",
            "âœ“ Scaler saved to: ml_classification_results/scaler.pkl\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\n",
            "================================================================================\n",
            "ðŸ“ All results saved to: ml_classification_results/\n",
            "ðŸ† Best Model: KNN\n",
            "ðŸ“Š F1-Score: 1.0000\n",
            "ðŸ“Š Accuracy: 1.0000\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              GradientBoostingClassifier, VotingClassifier, StackingClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "\n",
        "# Advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"Warning: XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"Warning: LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"Warning: CatBoost not available\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# FILE PATHS - UPDATE THESE WITH YOUR ACTUAL FILE PATHS\n",
        "FILE_MAPPING = {\n",
        "    'simple': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_sp_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_sp_nolemma.xlsx'\n",
        "    },\n",
        "    'simple_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_sp_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_sp_lemma.xlsx'\n",
        "    },\n",
        "    'nltk': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_nltk_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_nltk_nolemma.xlsx'\n",
        "    },\n",
        "    'nltk_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_nltk_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_nltk_lemma.xlsx'\n",
        "    },\n",
        "    'spacy': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_spacy_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_spacy_nolemma.xlsx'\n",
        "    },\n",
        "    'spacy_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_spacy_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_spacy_lemma.xlsx'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Statistical test results file\n",
        "BEST_FEATURES_FILE = 'best_features_summary.xlsx'\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = 'ml_classification_results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_SEED = 42\n",
        "CV_FOLDS = 5\n",
        "TOP_N_MODELS_TO_TUNE = 3\n",
        "\n",
        "# Class labels (documented clearly)\n",
        "# 0 = AI-generated text\n",
        "# 1 = Human-written text\n",
        "AI_LABEL = 0\n",
        "HUMAN_LABEL = 1\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Class Labels: AI = {AI_LABEL}, Human = {HUMAN_LABEL}\")\n",
        "print(f\"Test Size: {TEST_SIZE}, CV Folds: {CV_FOLDS}, Random Seed: {RANDOM_SEED}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SOURCE NAME MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "def map_source_name(source_str):\n",
        "    \"\"\"Map source names from Excel to file mapping keys\"\"\"\n",
        "    mapping = {\n",
        "        'Spacy Token': 'spacy',\n",
        "        'Spacy Lemma': 'spacy_lemma',\n",
        "        'Simple Token': 'simple',\n",
        "        'Simple Lemma': 'simple_lemma',\n",
        "        'Nltk Token': 'nltk',\n",
        "        'Nltk Lemma': 'nltk_lemma'\n",
        "    }\n",
        "    return mapping.get(source_str, source_str.lower().replace(' ', '_'))\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def load_feature_recommendations(filepath):\n",
        "    \"\"\"Load and filter highly recommended features with their sources\"\"\"\n",
        "    print(f\"Loading feature recommendations from: {filepath}\")\n",
        "\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Filter only highly recommended features\n",
        "    highly_recommended = df[df['feature_recommendation'] == 'Highly Recommended'].copy()\n",
        "\n",
        "    print(f\"Total features in file: {len(df)}\")\n",
        "    print(f\"Highly recommended features: {len(highly_recommended)}\")\n",
        "\n",
        "    # Create feature-source mapping\n",
        "    feature_source_map = {}\n",
        "    for _, row in highly_recommended.iterrows():\n",
        "        feature_name = row['metric']\n",
        "        source = map_source_name(row['source'])\n",
        "        feature_source_map[feature_name] = source\n",
        "\n",
        "    print(f\"\\nHighly Recommended Features and Their Sources:\")\n",
        "    for feature, source in feature_source_map.items():\n",
        "        print(f\"  - {feature}: {source}\")\n",
        "\n",
        "    return feature_source_map\n",
        "\n",
        "def load_excel_data(filepath, max_rows=300):\n",
        "    \"\"\"Load Excel file and clean it\"\"\"\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Remove 'Average' row if exists\n",
        "    df = df[df['Graph'] != 'Average'].copy()\n",
        "\n",
        "    # Remove 'Graph' column\n",
        "    if 'Graph' in df.columns:\n",
        "        df = df.drop('Graph', axis=1)\n",
        "\n",
        "    # Keep only first max_rows\n",
        "    df = df.head(max_rows)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_mixed_source_dataset(feature_source_map, file_mapping):\n",
        "    \"\"\"\n",
        "    Build dataset by selecting each feature from its best-performing source\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BUILDING MIXED-SOURCE DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    ai_features = []\n",
        "    human_features = []\n",
        "\n",
        "    for feature_name, source in feature_source_map.items():\n",
        "        print(f\"\\nProcessing feature: {feature_name} from source: {source}\")\n",
        "\n",
        "        try:\n",
        "            # Load AI data\n",
        "            ai_file = file_mapping[source]['ai']\n",
        "            ai_df = load_excel_data(ai_file)\n",
        "\n",
        "            if feature_name not in ai_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {ai_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            ai_feature_data = ai_df[[feature_name]].copy()\n",
        "            ai_feature_data.columns = [feature_name]\n",
        "\n",
        "            # Load Human data\n",
        "            human_file = file_mapping[source]['human']\n",
        "            human_df = load_excel_data(human_file)\n",
        "\n",
        "            if feature_name not in human_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {human_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            human_feature_data = human_df[[feature_name]].copy()\n",
        "            human_feature_data.columns = [feature_name]\n",
        "\n",
        "            ai_features.append(ai_feature_data)\n",
        "            human_features.append(human_feature_data)\n",
        "\n",
        "            print(f\"  âœ“ Successfully loaded from both AI and Human files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {feature_name} from {source}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Combine all features\n",
        "    print(\"\\nCombining features...\")\n",
        "    ai_combined = pd.concat(ai_features, axis=1)\n",
        "    human_combined = pd.concat(human_features, axis=1)\n",
        "\n",
        "    # Add labels\n",
        "    ai_combined['label'] = AI_LABEL\n",
        "    human_combined['label'] = HUMAN_LABEL\n",
        "\n",
        "    # Combine AI and Human\n",
        "    final_df = pd.concat([ai_combined, human_combined], axis=0, ignore_index=True)\n",
        "\n",
        "    print(f\"\\nâœ“ Dataset built successfully!\")\n",
        "    print(f\"  Shape: {final_df.shape}\")\n",
        "    print(f\"  Features: {final_df.shape[1] - 1}\")\n",
        "    print(f\"  Samples: {final_df.shape[0]}\")\n",
        "    print(f\"  AI samples: {(final_df['label'] == AI_LABEL).sum()}\")\n",
        "    print(f\"  Human samples: {(final_df['label'] == HUMAN_LABEL).sum()}\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ============================================================================\n",
        "# PREPROCESSING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Handle missing values and prepare data\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('label', axis=1)\n",
        "    y = df['label']\n",
        "\n",
        "    print(f\"Initial shape: {X.shape}\")\n",
        "\n",
        "    # Handle -1 values (replace with column median or 0)\n",
        "    print(\"\\nHandling missing values (-1)...\")\n",
        "    for col in X.columns:\n",
        "        missing_count = (X[col] == -1).sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values\")\n",
        "            # Replace -1 with median (or 0 if all are -1)\n",
        "            median_val = X[X[col] != -1][col].median()\n",
        "            if pd.isna(median_val):\n",
        "                X[col] = X[col].replace(-1, 0)\n",
        "            else:\n",
        "                X[col] = X[col].replace(-1, median_val)\n",
        "\n",
        "    # Handle boolean columns (convert True/False to 1/0)\n",
        "    bool_cols = X.select_dtypes(include=['bool']).columns\n",
        "    if len(bool_cols) > 0:\n",
        "        print(f\"\\nConverting boolean columns to numeric: {list(bool_cols)}\")\n",
        "        X[bool_cols] = X[bool_cols].astype(int)\n",
        "\n",
        "    # Check for any remaining NaN or inf values\n",
        "    if X.isnull().any().any():\n",
        "        print(\"\\nWarning: NaN values found, filling with 0\")\n",
        "        X = X.fillna(0)\n",
        "\n",
        "    if np.isinf(X.values).any():\n",
        "        print(\"\\nWarning: Infinite values found, replacing with column max\")\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "        X = X.fillna(X.max())\n",
        "\n",
        "    print(f\"\\nâœ“ Preprocessing complete!\")\n",
        "    print(f\"  Final shape: {X.shape}\")\n",
        "    print(f\"  Class distribution:\")\n",
        "    print(f\"    AI (0): {(y == 0).sum()}\")\n",
        "    print(f\"    Human (1): {(y == 1).sum()}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def split_and_scale_data(X, y):\n",
        "    \"\"\"Split data and scale features\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SPLITTING AND SCALING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "\n",
        "    print(\"âœ“ Scaling complete (StandardScaler)\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "def get_all_models():\n",
        "    \"\"\"Return dictionary of all models to benchmark\"\"\"\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Ridge Classifier': RidgeClassifier(random_state=RANDOM_SEED),\n",
        "        'SGD Classifier': SGDClassifier(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "        'Random Forest': RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=RANDOM_SEED),\n",
        "        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=RANDOM_SEED),\n",
        "        'Gaussian NB': GaussianNB(),\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models['XGBoost'] = xgb.XGBClassifier(random_state=RANDOM_SEED, eval_metric='logloss')\n",
        "\n",
        "    # Add LightGBM if available\n",
        "    if LIGHTGBM_AVAILABLE:\n",
        "        models['LightGBM'] = lgb.LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)\n",
        "\n",
        "    # Add CatBoost if available\n",
        "    if CATBOOST_AVAILABLE:\n",
        "        models['CatBoost'] = cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=0)\n",
        "\n",
        "    return models\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_and_evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate a single model\"\"\"\n",
        "    print(f\"\\n{'â”€' * 80}\")\n",
        "    print(f\"Training: {name}\")\n",
        "    print(f\"{'â”€' * 80}\")\n",
        "\n",
        "    results = {'Model': name}\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    results['Train Time (s)'] = round(train_time, 3)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Metrics\n",
        "    results['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results['Precision'] = precision_score(y_test, y_pred)\n",
        "    results['Recall'] = recall_score(y_test, y_pred)\n",
        "    results['F1-Score'] = f1_score(y_test, y_pred)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        results['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        results['ROC-AUC'] = np.nan\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')\n",
        "    results['CV Mean'] = cv_scores.mean()\n",
        "    results['CV Std'] = cv_scores.std()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Accuracy: {results['Accuracy']:.4f}\")\n",
        "    print(f\"Precision: {results['Precision']:.4f}\")\n",
        "    print(f\"Recall: {results['Recall']:.4f}\")\n",
        "    print(f\"F1-Score: {results['F1-Score']:.4f}\")\n",
        "    print(f\"ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "    print(f\"CV Accuracy: {results['CV Mean']:.4f} (+/- {results['CV Std']:.4f})\")\n",
        "    print(f\"Training Time: {results['Train Time (s)']} seconds\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return results, model, y_pred, y_pred_proba, cm\n",
        "\n",
        "def benchmark_all_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate all models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING ALL MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    models = get_all_models()\n",
        "    results_list = []\n",
        "    trained_models = {}\n",
        "    predictions = {}\n",
        "    confusion_matrices = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            results, trained_model, y_pred, y_pred_proba, cm = train_and_evaluate_model(\n",
        "                name, model, X_train, X_test, y_train, y_test\n",
        "            )\n",
        "            results_list.append(results)\n",
        "            trained_models[name] = trained_model\n",
        "            predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "            confusion_matrices[name] = cm\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING COMPLETE - RESULTS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    results_file = os.path.join(OUTPUT_DIR, 'model_comparison_results.xlsx')\n",
        "    results_df.to_excel(results_file, index=False)\n",
        "    print(f\"\\nâœ“ Results saved to: {results_file}\")\n",
        "\n",
        "    return results_df, trained_models, predictions, confusion_matrices\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING\n",
        "# ============================================================================\n",
        "\n",
        "def get_param_grids():\n",
        "    \"\"\"Define parameter grids for hyperparameter tuning\"\"\"\n",
        "    param_grids = {\n",
        "        'Random Forest': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'num_leaves': [31, 50, 70]\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs', 'liblinear']\n",
        "        },\n",
        "        'SVM (RBF)': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return param_grids\n",
        "\n",
        "def tune_top_models(top_model_names, trained_models, X_train, y_train):\n",
        "    \"\"\"Perform hyperparameter tuning on top models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"HYPERPARAMETER TUNING - TOP {len(top_model_names)} MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    param_grids = get_param_grids()\n",
        "    tuned_models = {}\n",
        "    tuning_results = []\n",
        "\n",
        "    for name in top_model_names:\n",
        "        if name not in param_grids:\n",
        "            print(f\"\\nNo parameter grid defined for {name}, skipping tuning\")\n",
        "            tuned_models[name] = trained_models[name]\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'â”€' * 80}\")\n",
        "        print(f\"Tuning: {name}\")\n",
        "        print(f\"{'â”€' * 80}\")\n",
        "\n",
        "        base_model = trained_models[name]\n",
        "        param_grid = param_grids[name]\n",
        "\n",
        "        # Use RandomizedSearchCV for faster tuning\n",
        "        search = RandomizedSearchCV(\n",
        "            base_model, param_grid, n_iter=20, cv=CV_FOLDS,\n",
        "            scoring='f1', random_state=RANDOM_SEED, n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        search.fit(X_train, y_train)\n",
        "        tune_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nBest parameters: {search.best_params_}\")\n",
        "        print(f\"Best CV F1-Score: {search.best_score_:.4f}\")\n",
        "        print(f\"Tuning time: {tune_time:.2f} seconds\")\n",
        "\n",
        "        tuned_models[name] = search.best_estimator_\n",
        "\n",
        "        tuning_results.append({\n",
        "            'Model': name,\n",
        "            'Best CV F1-Score': search.best_score_,\n",
        "            'Best Parameters': str(search.best_params_),\n",
        "            'Tuning Time (s)': round(tune_time, 2)\n",
        "        })\n",
        "\n",
        "    # Save tuning results\n",
        "    tuning_df = pd.DataFrame(tuning_results)\n",
        "    tuning_file = os.path.join(OUTPUT_DIR, 'top_models_tuning_results.xlsx')\n",
        "    tuning_df.to_excel(tuning_file, index=False)\n",
        "    print(f\"\\nâœ“ Tuning results saved to: {tuning_file}\")\n",
        "\n",
        "    return tuned_models\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def final_evaluation(tuned_models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Evaluate tuned models on test set\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL EVALUATION OF TUNED MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    final_results = []\n",
        "    final_predictions = {}\n",
        "    final_cms = {}\n",
        "\n",
        "    for name, model in tuned_models.items():\n",
        "        print(f\"\\nEvaluating: {name}\")\n",
        "\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Metrics\n",
        "        results = {\n",
        "            'Model': name,\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Precision': precision_score(y_test, y_pred),\n",
        "            'Recall': recall_score(y_test, y_pred),\n",
        "            'F1-Score': f1_score(y_test, y_pred),\n",
        "            'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else np.nan\n",
        "        }\n",
        "\n",
        "        final_results.append(results)\n",
        "        final_predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "        final_cms[name] = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Accuracy: {results['Accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score: {results['F1-Score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "\n",
        "    final_df = pd.DataFrame(final_results).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(final_df.to_string(index=False))\n",
        "\n",
        "    # Save final results\n",
        "    final_file = os.path.join(OUTPUT_DIR, 'final_tuned_models_results.xlsx')\n",
        "    final_df.to_excel(final_file, index=False)\n",
        "    print(f\"\\nâœ“ Final results saved to: {final_file}\")\n",
        "\n",
        "    # Get best model\n",
        "    best_model_name = final_df.iloc[0]['Model']\n",
        "    best_model = tuned_models[best_model_name]\n",
        "\n",
        "    print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"   Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    model_file = os.path.join(OUTPUT_DIR, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
        "    joblib.dump(best_model, model_file)\n",
        "    print(f\"\\nâœ“ Best model saved to: {model_file}\")\n",
        "\n",
        "    return final_df, best_model, best_model_name, final_predictions, final_cms\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_model_comparison(results_df, save_path):\n",
        "    \"\"\"Plot model performance comparison\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        data = results_df.sort_values(metric, ascending=True)\n",
        "\n",
        "        ax.barh(data['Model'], data[metric], color='steelblue')\n",
        "        ax.set_xlabel(metric, fontsize=12)\n",
        "        ax.set_title(f'{metric} by Model', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlim([0, 1])\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(data[metric]):\n",
        "            ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Comparison plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrices(confusion_matrices, save_path):\n",
        "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "    n_models = len(confusion_matrices)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_models > 1 else [axes]\n",
        "\n",
        "    for idx, (name, cm) in enumerate(confusion_matrices.items()):\n",
        "        ax = axes[idx]\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                   xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
        "        ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_models, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Confusion matrices saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curves(predictions, y_test, save_path):\n",
        "    \"\"\"Plot ROC curves for all models\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for name, preds in predictions.items():\n",
        "        if preds['y_pred_proba'] is not None:\n",
        "            fpr, tpr, _ = roc_curve(y_test, preds['y_pred_proba'])\n",
        "            auc = roc_auc_score(y_test, preds['y_pred_proba'])\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='lower right', fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ ROC curves saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(model, feature_names, model_name, save_path):\n",
        "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "    if not hasattr(model, 'feature_importances_'):\n",
        "        print(f\"Feature importance not available for {model_name}\")\n",
        "        return\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:20]  # Top 20\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(len(indices)), importances[indices], color='steelblue')\n",
        "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance', fontsize=12)\n",
        "    plt.title(f'Top 20 Feature Importance - {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Feature importance plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save to Excel\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    excel_path = save_path.replace('.png', '.xlsx')\n",
        "    importance_df.to_excel(excel_path, index=False)\n",
        "    print(f\"âœ“ Feature importance data saved to: {excel_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    # Step 1: Load feature recommendations\n",
        "    feature_source_map = load_feature_recommendations(BEST_FEATURES_FILE)\n",
        "\n",
        "    # Step 2: Build mixed-source dataset\n",
        "    df = build_mixed_source_dataset(feature_source_map, FILE_MAPPING)\n",
        "\n",
        "    # Step 3: Preprocess data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Split and scale\n",
        "    X_train, X_test, y_train, y_test, scaler = split_and_scale_data(X, y)\n",
        "\n",
        "    # Step 5: Benchmark all models\n",
        "    results_df, trained_models, predictions, confusion_matrices = benchmark_all_models(\n",
        "        X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 6: Visualizations for all models\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    plot_model_comparison(\n",
        "        results_df,\n",
        "        os.path.join(OUTPUT_DIR, 'model_comparison.png')\n",
        "    )\n",
        "\n",
        "    plot_confusion_matrices(\n",
        "        confusion_matrices,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_all.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        predictions,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_all.png')\n",
        "    )\n",
        "\n",
        "    # Step 7: Select top models for tuning\n",
        "    top_models = results_df.head(TOP_N_MODELS_TO_TUNE)['Model'].tolist()\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(f\"TOP {TOP_N_MODELS_TO_TUNE} MODELS FOR HYPERPARAMETER TUNING:\")\n",
        "    for i, model_name in enumerate(top_models, 1):\n",
        "        print(f\"  {i}. {model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 8: Hyperparameter tuning\n",
        "    tuned_models = tune_top_models(top_models, trained_models, X_train, y_train)\n",
        "\n",
        "    # Step 9: Final evaluation\n",
        "    final_df, best_model, best_model_name, final_preds, final_cms = final_evaluation(\n",
        "        tuned_models, X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 10: Final visualizations\n",
        "    plot_confusion_matrices(\n",
        "        final_cms,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_tuned.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        final_preds,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_tuned.png')\n",
        "    )\n",
        "\n",
        "    # Step 11: Feature importance for best model\n",
        "    plot_feature_importance(\n",
        "        best_model,\n",
        "        X.columns.tolist(),\n",
        "        best_model_name,\n",
        "        os.path.join(OUTPUT_DIR, f'feature_importance_{best_model_name.replace(\" \", \"_\")}.png')\n",
        "    )\n",
        "\n",
        "    # Step 12: Generate classification report for best model\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred_best, target_names=['AI', 'Human']))\n",
        "\n",
        "    # Save classification report\n",
        "    report_dict = classification_report(y_test, y_pred_best, target_names=['AI', 'Human'], output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    report_file = os.path.join(OUTPUT_DIR, f'classification_report_{best_model_name.replace(\" \", \"_\")}.xlsx')\n",
        "    report_df.to_excel(report_file)\n",
        "    print(f\"\\nâœ“ Classification report saved to: {report_file}\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_file = os.path.join(OUTPUT_DIR, 'scaler.pkl')\n",
        "    joblib.dump(scaler, scaler_file)\n",
        "    print(f\"âœ“ Scaler saved to: {scaler_file}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“ All results saved to: {OUTPUT_DIR}/\")\n",
        "    print(f\"ðŸ† Best Model: {best_model_name}\")\n",
        "    print(f\"ðŸ“Š F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"ðŸ“Š Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              GradientBoostingClassifier, VotingClassifier, StackingClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "\n",
        "# Advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"Warning: XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"Warning: LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"Warning: CatBoost not available\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# FILE PATHS - UPDATE THESE WITH YOUR ACTUAL FILE PATHS\n",
        "FILE_MAPPING = {\n",
        "    'simple': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_sp_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_sp_nolemma.xlsx'\n",
        "    },\n",
        "    'simple_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_sp_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_sp_lemma.xlsx'\n",
        "    },\n",
        "    'nltk': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_nltk_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_nltk_nolemma.xlsx'\n",
        "    },\n",
        "    'nltk_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_nltk_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_nltk_lemma.xlsx'\n",
        "    },\n",
        "    'spacy': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_spacy_nolemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_spacy_nolemma.xlsx'\n",
        "    },\n",
        "    'spacy_lemma': {\n",
        "        'ai': 'AI_english_pmi_graph_analysis_adv_spacy_lemma.xlsx',\n",
        "        'human': 'Human_english_pmi_graph_analysis_adv_spacy_lemma.xlsx'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Statistical test results file\n",
        "BEST_FEATURES_FILE = 'best_features_summary.xlsx'\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = 'ml_classification_results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_SEED = 42\n",
        "CV_FOLDS = 5\n",
        "TOP_N_MODELS_TO_TUNE = 10\n",
        "\n",
        "# Class labels (documented clearly)\n",
        "# 0 = AI-generated text\n",
        "# 1 = Human-written text\n",
        "AI_LABEL = 0\n",
        "HUMAN_LABEL = 1\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Class Labels: AI = {AI_LABEL}, Human = {HUMAN_LABEL}\")\n",
        "print(f\"Test Size: {TEST_SIZE}, CV Folds: {CV_FOLDS}, Random Seed: {RANDOM_SEED}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SOURCE NAME MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "def map_source_name(source_str):\n",
        "    \"\"\"Map source names from Excel to file mapping keys\"\"\"\n",
        "    mapping = {\n",
        "        'Spacy Token': 'spacy',\n",
        "        'Spacy Lemma': 'spacy_lemma',\n",
        "        'Simple Token': 'simple',\n",
        "        'Simple Lemma': 'simple_lemma',\n",
        "        'Nltk Token': 'nltk',\n",
        "        'Nltk Lemma': 'nltk_lemma'\n",
        "    }\n",
        "    return mapping.get(source_str, source_str.lower().replace(' ', '_'))\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def load_feature_recommendations(filepath):\n",
        "    \"\"\"Load and filter highly recommended features with their sources\"\"\"\n",
        "    print(f\"Loading feature recommendations from: {filepath}\")\n",
        "\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Filter only highly recommended features\n",
        "    highly_recommended = df[df['feature_recommendation'] == 'Highly Recommended'].copy()\n",
        "\n",
        "    print(f\"Total features in file: {len(df)}\")\n",
        "    print(f\"Highly recommended features: {len(highly_recommended)}\")\n",
        "\n",
        "    # Create feature-source mapping\n",
        "    feature_source_map = {}\n",
        "    for _, row in highly_recommended.iterrows():\n",
        "        feature_name = row['metric']\n",
        "        source = map_source_name(row['source'])\n",
        "        feature_source_map[feature_name] = source\n",
        "\n",
        "    print(f\"\\nHighly Recommended Features and Their Sources:\")\n",
        "    for feature, source in feature_source_map.items():\n",
        "        print(f\"  - {feature}: {source}\")\n",
        "\n",
        "    return feature_source_map\n",
        "\n",
        "def load_excel_data(filepath, max_rows=300):\n",
        "    \"\"\"Load Excel file and clean it\"\"\"\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Remove 'Average' row if exists\n",
        "    df = df[df['Graph'] != 'Average'].copy()\n",
        "\n",
        "    # Remove 'Graph' column\n",
        "    if 'Graph' in df.columns:\n",
        "        df = df.drop('Graph', axis=1)\n",
        "\n",
        "    # Keep only first max_rows\n",
        "    df = df.head(max_rows)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_mixed_source_dataset(feature_source_map, file_mapping):\n",
        "    \"\"\"\n",
        "    Build dataset by selecting each feature from its best-performing source\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BUILDING MIXED-SOURCE DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    ai_features = []\n",
        "    human_features = []\n",
        "\n",
        "    for feature_name, source in feature_source_map.items():\n",
        "        print(f\"\\nProcessing feature: {feature_name} from source: {source}\")\n",
        "\n",
        "        try:\n",
        "            # Load AI data\n",
        "            ai_file = file_mapping[source]['ai']\n",
        "            ai_df = load_excel_data(ai_file)\n",
        "\n",
        "            if feature_name not in ai_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {ai_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            ai_feature_data = ai_df[[feature_name]].copy()\n",
        "            ai_feature_data.columns = [feature_name]\n",
        "\n",
        "            # Load Human data\n",
        "            human_file = file_mapping[source]['human']\n",
        "            human_df = load_excel_data(human_file)\n",
        "\n",
        "            if feature_name not in human_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {human_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            human_feature_data = human_df[[feature_name]].copy()\n",
        "            human_feature_data.columns = [feature_name]\n",
        "\n",
        "            ai_features.append(ai_feature_data)\n",
        "            human_features.append(human_feature_data)\n",
        "\n",
        "            print(f\"  âœ“ Successfully loaded from both AI and Human files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {feature_name} from {source}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Combine all features\n",
        "    print(\"\\nCombining features...\")\n",
        "    ai_combined = pd.concat(ai_features, axis=1)\n",
        "    human_combined = pd.concat(human_features, axis=1)\n",
        "\n",
        "    # Add labels\n",
        "    ai_combined['label'] = AI_LABEL\n",
        "    human_combined['label'] = HUMAN_LABEL\n",
        "\n",
        "    # Combine AI and Human\n",
        "    final_df = pd.concat([ai_combined, human_combined], axis=0, ignore_index=True)\n",
        "\n",
        "    print(f\"\\nâœ“ Dataset built successfully!\")\n",
        "    print(f\"  Shape: {final_df.shape}\")\n",
        "    print(f\"  Features: {final_df.shape[1] - 1}\")\n",
        "    print(f\"  Samples: {final_df.shape[0]}\")\n",
        "    print(f\"  AI samples: {(final_df['label'] == AI_LABEL).sum()}\")\n",
        "    print(f\"  Human samples: {(final_df['label'] == HUMAN_LABEL).sum()}\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ============================================================================\n",
        "# PREPROCESSING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Handle missing values and prepare data\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('label', axis=1)\n",
        "    y = df['label']\n",
        "\n",
        "    print(f\"Initial shape: {X.shape}\")\n",
        "\n",
        "    # Handle -1 values (replace with column median or 0)\n",
        "    print(\"\\nHandling missing values (-1)...\")\n",
        "    for col in X.columns:\n",
        "        missing_count = (X[col] == -1).sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values\")\n",
        "            # Replace -1 with median (or 0 if all are -1)\n",
        "            median_val = X[X[col] != -1][col].median()\n",
        "            if pd.isna(median_val):\n",
        "                X[col] = X[col].replace(-1, 0)\n",
        "            else:\n",
        "                X[col] = X[col].replace(-1, median_val)\n",
        "\n",
        "    # Handle boolean columns (convert True/False to 1/0)\n",
        "    bool_cols = X.select_dtypes(include=['bool']).columns\n",
        "    if len(bool_cols) > 0:\n",
        "        print(f\"\\nConverting boolean columns to numeric: {list(bool_cols)}\")\n",
        "        X[bool_cols] = X[bool_cols].astype(int)\n",
        "\n",
        "    # Check for any remaining NaN or inf values\n",
        "    if X.isnull().any().any():\n",
        "        print(\"\\nWarning: NaN values found, filling with 0\")\n",
        "        X = X.fillna(0)\n",
        "\n",
        "    if np.isinf(X.values).any():\n",
        "        print(\"\\nWarning: Infinite values found, replacing with column max\")\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "        X = X.fillna(X.max())\n",
        "\n",
        "    print(f\"\\nâœ“ Preprocessing complete!\")\n",
        "    print(f\"  Final shape: {X.shape}\")\n",
        "    print(f\"  Class distribution:\")\n",
        "    print(f\"    AI (0): {(y == 0).sum()}\")\n",
        "    print(f\"    Human (1): {(y == 1).sum()}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def split_and_scale_data(X, y):\n",
        "    \"\"\"Split data and scale features\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SPLITTING AND SCALING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "\n",
        "    print(\"âœ“ Scaling complete (StandardScaler)\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "def get_all_models():\n",
        "    \"\"\"Return dictionary of all models to benchmark\"\"\"\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Ridge Classifier': RidgeClassifier(random_state=RANDOM_SEED),\n",
        "        'SGD Classifier': SGDClassifier(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "        'Random Forest': RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=RANDOM_SEED),\n",
        "        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=RANDOM_SEED),\n",
        "        'Gaussian NB': GaussianNB(),\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models['XGBoost'] = xgb.XGBClassifier(random_state=RANDOM_SEED, eval_metric='logloss')\n",
        "\n",
        "    # Add LightGBM if available\n",
        "    if LIGHTGBM_AVAILABLE:\n",
        "        models['LightGBM'] = lgb.LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)\n",
        "\n",
        "    # Add CatBoost if available\n",
        "    if CATBOOST_AVAILABLE:\n",
        "        models['CatBoost'] = cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=0)\n",
        "\n",
        "    return models\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_and_evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate a single model\"\"\"\n",
        "    print(f\"\\n{'â”€' * 80}\")\n",
        "    print(f\"Training: {name}\")\n",
        "    print(f\"{'â”€' * 80}\")\n",
        "\n",
        "    results = {'Model': name}\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    results['Train Time (s)'] = round(train_time, 3)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Metrics\n",
        "    results['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results['Precision'] = precision_score(y_test, y_pred)\n",
        "    results['Recall'] = recall_score(y_test, y_pred)\n",
        "    results['F1-Score'] = f1_score(y_test, y_pred)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        results['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        results['ROC-AUC'] = np.nan\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')\n",
        "    results['CV Mean'] = cv_scores.mean()\n",
        "    results['CV Std'] = cv_scores.std()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Accuracy: {results['Accuracy']:.4f}\")\n",
        "    print(f\"Precision: {results['Precision']:.4f}\")\n",
        "    print(f\"Recall: {results['Recall']:.4f}\")\n",
        "    print(f\"F1-Score: {results['F1-Score']:.4f}\")\n",
        "    print(f\"ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "    print(f\"CV Accuracy: {results['CV Mean']:.4f} (+/- {results['CV Std']:.4f})\")\n",
        "    print(f\"Training Time: {results['Train Time (s)']} seconds\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return results, model, y_pred, y_pred_proba, cm\n",
        "\n",
        "def benchmark_all_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate all models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING ALL MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    models = get_all_models()\n",
        "    results_list = []\n",
        "    trained_models = {}\n",
        "    predictions = {}\n",
        "    confusion_matrices = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            results, trained_model, y_pred, y_pred_proba, cm = train_and_evaluate_model(\n",
        "                name, model, X_train, X_test, y_train, y_test\n",
        "            )\n",
        "            results_list.append(results)\n",
        "            trained_models[name] = trained_model\n",
        "            predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "            confusion_matrices[name] = cm\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING COMPLETE - RESULTS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    results_file = os.path.join(OUTPUT_DIR, 'model_comparison_results.xlsx')\n",
        "    results_df.to_excel(results_file, index=False)\n",
        "    print(f\"\\nâœ“ Results saved to: {results_file}\")\n",
        "\n",
        "    return results_df, trained_models, predictions, confusion_matrices\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING\n",
        "# ============================================================================\n",
        "\n",
        "def get_param_grids():\n",
        "    \"\"\"Define parameter grids for hyperparameter tuning\"\"\"\n",
        "    param_grids = {\n",
        "        'Random Forest': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'num_leaves': [31, 50, 70]\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs', 'liblinear']\n",
        "        },\n",
        "        'SVM (RBF)': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return param_grids\n",
        "\n",
        "def tune_top_models(top_model_names, trained_models, X_train, y_train):\n",
        "    \"\"\"Perform hyperparameter tuning on top models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"HYPERPARAMETER TUNING - TOP {len(top_model_names)} MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    param_grids = get_param_grids()\n",
        "    tuned_models = {}\n",
        "    tuning_results = []\n",
        "\n",
        "    for name in top_model_names:\n",
        "        if name not in param_grids:\n",
        "            print(f\"\\nNo parameter grid defined for {name}, skipping tuning\")\n",
        "            tuned_models[name] = trained_models[name]\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'â”€' * 80}\")\n",
        "        print(f\"Tuning: {name}\")\n",
        "        print(f\"{'â”€' * 80}\")\n",
        "\n",
        "        base_model = trained_models[name]\n",
        "        param_grid = param_grids[name]\n",
        "\n",
        "        # Use RandomizedSearchCV for faster tuning\n",
        "        search = RandomizedSearchCV(\n",
        "            base_model, param_grid, n_iter=20, cv=CV_FOLDS,\n",
        "            scoring='f1', random_state=RANDOM_SEED, n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        search.fit(X_train, y_train)\n",
        "        tune_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nBest parameters: {search.best_params_}\")\n",
        "        print(f\"Best CV F1-Score: {search.best_score_:.4f}\")\n",
        "        print(f\"Tuning time: {tune_time:.2f} seconds\")\n",
        "\n",
        "        tuned_models[name] = search.best_estimator_\n",
        "\n",
        "        tuning_results.append({\n",
        "            'Model': name,\n",
        "            'Best CV F1-Score': search.best_score_,\n",
        "            'Best Parameters': str(search.best_params_),\n",
        "            'Tuning Time (s)': round(tune_time, 2)\n",
        "        })\n",
        "\n",
        "    # Save tuning results\n",
        "    tuning_df = pd.DataFrame(tuning_results)\n",
        "    tuning_file = os.path.join(OUTPUT_DIR, 'top_models_tuning_results.xlsx')\n",
        "    tuning_df.to_excel(tuning_file, index=False)\n",
        "    print(f\"\\nâœ“ Tuning results saved to: {tuning_file}\")\n",
        "\n",
        "    return tuned_models\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def final_evaluation(tuned_models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Evaluate tuned models on test set\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL EVALUATION OF TUNED MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    final_results = []\n",
        "    final_predictions = {}\n",
        "    final_cms = {}\n",
        "\n",
        "    for name, model in tuned_models.items():\n",
        "        print(f\"\\nEvaluating: {name}\")\n",
        "\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Metrics\n",
        "        results = {\n",
        "            'Model': name,\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Precision': precision_score(y_test, y_pred),\n",
        "            'Recall': recall_score(y_test, y_pred),\n",
        "            'F1-Score': f1_score(y_test, y_pred),\n",
        "            'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else np.nan\n",
        "        }\n",
        "\n",
        "        final_results.append(results)\n",
        "        final_predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "        final_cms[name] = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Accuracy: {results['Accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score: {results['F1-Score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "\n",
        "    final_df = pd.DataFrame(final_results).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(final_df.to_string(index=False))\n",
        "\n",
        "    # Save final results\n",
        "    final_file = os.path.join(OUTPUT_DIR, 'final_tuned_models_results.xlsx')\n",
        "    final_df.to_excel(final_file, index=False)\n",
        "    print(f\"\\nâœ“ Final results saved to: {final_file}\")\n",
        "\n",
        "    # Get best model\n",
        "    best_model_name = final_df.iloc[0]['Model']\n",
        "    best_model = tuned_models[best_model_name]\n",
        "\n",
        "    print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"   Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    model_file = os.path.join(OUTPUT_DIR, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
        "    joblib.dump(best_model, model_file)\n",
        "    print(f\"\\nâœ“ Best model saved to: {model_file}\")\n",
        "\n",
        "    return final_df, best_model, best_model_name, final_predictions, final_cms\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_model_comparison(results_df, save_path):\n",
        "    \"\"\"Plot model performance comparison\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        data = results_df.sort_values(metric, ascending=True)\n",
        "\n",
        "        ax.barh(data['Model'], data[metric], color='steelblue')\n",
        "        ax.set_xlabel(metric, fontsize=12)\n",
        "        ax.set_title(f'{metric} by Model', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlim([0, 1])\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(data[metric]):\n",
        "            ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Comparison plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrices(confusion_matrices, save_path):\n",
        "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "    n_models = len(confusion_matrices)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_models > 1 else [axes]\n",
        "\n",
        "    for idx, (name, cm) in enumerate(confusion_matrices.items()):\n",
        "        ax = axes[idx]\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                   xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
        "        ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_models, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Confusion matrices saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curves(predictions, y_test, save_path):\n",
        "    \"\"\"Plot ROC curves for all models\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for name, preds in predictions.items():\n",
        "        if preds['y_pred_proba'] is not None:\n",
        "            fpr, tpr, _ = roc_curve(y_test, preds['y_pred_proba'])\n",
        "            auc = roc_auc_score(y_test, preds['y_pred_proba'])\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='lower right', fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ ROC curves saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(model, feature_names, model_name, save_path):\n",
        "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "    if not hasattr(model, 'feature_importances_'):\n",
        "        print(f\"Feature importance not available for {model_name}\")\n",
        "        return\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:20]  # Top 20\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(len(indices)), importances[indices], color='steelblue')\n",
        "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance', fontsize=12)\n",
        "    plt.title(f'Top 20 Feature Importance - {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Feature importance plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save to Excel\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    excel_path = save_path.replace('.png', '.xlsx')\n",
        "    importance_df.to_excel(excel_path, index=False)\n",
        "    print(f\"âœ“ Feature importance data saved to: {excel_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    # Step 1: Load feature recommendations\n",
        "    feature_source_map = load_feature_recommendations(BEST_FEATURES_FILE)\n",
        "\n",
        "    # Step 2: Build mixed-source dataset\n",
        "    df = build_mixed_source_dataset(feature_source_map, FILE_MAPPING)\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE FINAL COMBINED DATASET - ADD THIS CODE HERE\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAVING FINAL COMBINED DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Save the complete dataset (before preprocessing)\n",
        "    combined_dataset_file = os.path.join(OUTPUT_DIR, 'final_combined_dataset.xlsx')\n",
        "    df.to_excel(combined_dataset_file, index=False)\n",
        "    print(f\"âœ“ Final combined dataset saved to: {combined_dataset_file}\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Also save as CSV for easier access\n",
        "    combined_dataset_csv = os.path.join(OUTPUT_DIR, 'final_combined_dataset.csv')\n",
        "    df.to_csv(combined_dataset_csv, index=False)\n",
        "    print(f\"âœ“ Final combined dataset (CSV) saved to: {combined_dataset_csv}\")\n",
        "\n",
        "    # Save a summary of data sources for each feature\n",
        "    feature_source_summary = pd.DataFrame([\n",
        "        {'Feature': feature, 'Source': source}\n",
        "        for feature, source in feature_source_map.items()\n",
        "    ])\n",
        "    source_summary_file = os.path.join(OUTPUT_DIR, 'feature_source_mapping.xlsx')\n",
        "    feature_source_summary.to_excel(source_summary_file, index=False)\n",
        "    print(f\"âœ“ Feature-source mapping saved to: {source_summary_file}\")\n",
        "    print(\"=\" * 80)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Step 3: Preprocess data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Split and scale\n",
        "    X_train, X_test, y_train, y_test, scaler = split_and_scale_data(X, y)\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE PREPROCESSED DATASETS - ADD THIS CODE HERE (OPTIONAL)\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAVING PREPROCESSED DATASETS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Save train and test sets (before scaling)\n",
        "    train_df = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_df = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    train_file = os.path.join(OUTPUT_DIR, 'train_set.xlsx')\n",
        "    test_file = os.path.join(OUTPUT_DIR, 'test_set.xlsx')\n",
        "\n",
        "    train_df.to_excel(train_file, index=False)\n",
        "    test_df.to_excel(test_file, index=False)\n",
        "\n",
        "    print(f\"âœ“ Training set saved to: {train_file}\")\n",
        "    print(f\"  Shape: {train_df.shape}\")\n",
        "    print(f\"âœ“ Test set saved to: {test_file}\")\n",
        "    print(f\"  Shape: {test_df.shape}\")\n",
        "    print(\"=\" * 80)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Step 5: Benchmark all models\n",
        "    results_df, trained_models, predictions, confusion_matrices = benchmark_all_models(\n",
        "        X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 6: Visualizations for all models\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    plot_model_comparison(\n",
        "        results_df,\n",
        "        os.path.join(OUTPUT_DIR, 'model_comparison.png')\n",
        "    )\n",
        "\n",
        "    plot_confusion_matrices(\n",
        "        confusion_matrices,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_all.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        predictions,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_all.png')\n",
        "    )\n",
        "\n",
        "    # Step 7: Select top models for tuning\n",
        "    top_models = results_df.head(TOP_N_MODELS_TO_TUNE)['Model'].tolist()\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(f\"TOP {TOP_N_MODELS_TO_TUNE} MODELS FOR HYPERPARAMETER TUNING:\")\n",
        "    for i, model_name in enumerate(top_models, 1):\n",
        "        print(f\"  {i}. {model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 8: Hyperparameter tuning\n",
        "    tuned_models = tune_top_models(top_models, trained_models, X_train, y_train)\n",
        "\n",
        "    # Step 9: Final evaluation\n",
        "    final_df, best_model, best_model_name, final_preds, final_cms = final_evaluation(\n",
        "        tuned_models, X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 10: Final visualizations\n",
        "    plot_confusion_matrices(\n",
        "        final_cms,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_tuned.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        final_preds,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_tuned.png')\n",
        "    )\n",
        "\n",
        "    # Step 11: Feature importance for best model\n",
        "    plot_feature_importance(\n",
        "        best_model,\n",
        "        X.columns.tolist(),\n",
        "        best_model_name,\n",
        "        os.path.join(OUTPUT_DIR, f'feature_importance_{best_model_name.replace(\" \", \"_\")}.png')\n",
        "    )\n",
        "\n",
        "    # Step 12: Generate classification report for best model\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred_best, target_names=['AI', 'Human']))\n",
        "\n",
        "    # Save classification report\n",
        "    report_dict = classification_report(y_test, y_pred_best, target_names=['AI', 'Human'], output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    report_file = os.path.join(OUTPUT_DIR, f'classification_report_{best_model_name.replace(\" \", \"_\")}.xlsx')\n",
        "    report_df.to_excel(report_file)\n",
        "    print(f\"\\nâœ“ Classification report saved to: {report_file}\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_file = os.path.join(OUTPUT_DIR, 'scaler.pkl')\n",
        "    joblib.dump(scaler, scaler_file)\n",
        "    print(f\"âœ“ Scaler saved to: {scaler_file}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“ All results saved to: {OUTPUT_DIR}/\")\n",
        "    print(f\"ðŸ† Best Model: {best_model_name}\")\n",
        "    print(f\"ðŸ“Š F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"ðŸ“Š Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYhWgajXM1IP",
        "outputId": "d6a7b31b-c508-4bb0-cf63-110e0b42bdd7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\n",
            "================================================================================\n",
            "Class Labels: AI = 0, Human = 1\n",
            "Test Size: 0.2, CV Folds: 5, Random Seed: 42\n",
            "================================================================================\n",
            "\n",
            "Loading feature recommendations from: best_features_summary.xlsx\n",
            "Total features in file: 21\n",
            "Highly recommended features: 16\n",
            "\n",
            "Highly Recommended Features and Their Sources:\n",
            "  - transitivity: nltk\n",
            "  - top_eigenvalue: nltk_lemma\n",
            "  - avg_closeness_centrality: nltk\n",
            "  - avg_degree_centrality: nltk\n",
            "  - density: nltk\n",
            "  - algebraic_connectivity: nltk_lemma\n",
            "  - matching_number: nltk\n",
            "  - size_largest_component: nltk\n",
            "  - number_of_nodes: nltk\n",
            "  - graph_energy: nltk\n",
            "  - wiener_index: nltk\n",
            "  - avg_shortest_path_length: spacy\n",
            "  - radius: spacy\n",
            "  - diameter: simple\n",
            "  - second_eigenvalue: nltk\n",
            "  - avg_betweenness_centrality: spacy\n",
            "\n",
            "================================================================================\n",
            "BUILDING MIXED-SOURCE DATASET\n",
            "================================================================================\n",
            "\n",
            "Processing feature: transitivity from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: top_eigenvalue from source: nltk_lemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_closeness_centrality from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_degree_centrality from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: density from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: algebraic_connectivity from source: nltk_lemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: matching_number from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: size_largest_component from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: number_of_nodes from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: graph_energy from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: wiener_index from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_shortest_path_length from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: radius from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: diameter from source: simple\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: second_eigenvalue from source: nltk\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_betweenness_centrality from source: spacy\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Combining features...\n",
            "\n",
            "âœ“ Dataset built successfully!\n",
            "  Shape: (600, 17)\n",
            "  Features: 16\n",
            "  Samples: 600\n",
            "  AI samples: 300\n",
            "  Human samples: 300\n",
            "\n",
            "================================================================================\n",
            "SAVING FINAL COMBINED DATASET\n",
            "================================================================================\n",
            "âœ“ Final combined dataset saved to: ml_classification_results/final_combined_dataset.xlsx\n",
            "  Shape: (600, 17)\n",
            "  Columns: ['transitivity', 'top_eigenvalue', 'avg_closeness_centrality', 'avg_degree_centrality', 'density', 'algebraic_connectivity', 'matching_number', 'size_largest_component', 'number_of_nodes', 'graph_energy', 'wiener_index', 'avg_shortest_path_length', 'radius', 'diameter', 'second_eigenvalue', 'avg_betweenness_centrality', 'label']\n",
            "âœ“ Final combined dataset (CSV) saved to: ml_classification_results/final_combined_dataset.csv\n",
            "âœ“ Feature-source mapping saved to: ml_classification_results/feature_source_mapping.xlsx\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING DATA\n",
            "================================================================================\n",
            "Initial shape: (600, 16)\n",
            "\n",
            "Handling missing values (-1)...\n",
            "\n",
            "âœ“ Preprocessing complete!\n",
            "  Final shape: (600, 16)\n",
            "  Class distribution:\n",
            "    AI (0): 300\n",
            "    Human (1): 300\n",
            "\n",
            "================================================================================\n",
            "SPLITTING AND SCALING DATA\n",
            "================================================================================\n",
            "Train set: 480 samples\n",
            "Test set: 120 samples\n",
            "âœ“ Scaling complete (StandardScaler)\n",
            "\n",
            "================================================================================\n",
            "SAVING PREPROCESSED DATASETS\n",
            "================================================================================\n",
            "âœ“ Training set saved to: ml_classification_results/train_set.xlsx\n",
            "  Shape: (480, 17)\n",
            "âœ“ Test set saved to: ml_classification_results/test_set.xlsx\n",
            "  Shape: (120, 17)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Logistic Regression\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9792 (+/- 0.0066)\n",
            "Training Time: 0.006 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Ridge Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.9771 (+/- 0.0078)\n",
            "Training Time: 0.004 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SGD Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9583\n",
            "Precision: 0.9231\n",
            "Recall: 1.0000\n",
            "F1-Score: 0.9600\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.9583 (+/- 0.0219)\n",
            "Training Time: 0.003 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Decision Tree\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9583\n",
            "Precision: 0.9661\n",
            "Recall: 0.9500\n",
            "F1-Score: 0.9580\n",
            "ROC-AUC: 0.9583\n",
            "CV Accuracy: 0.9521 (+/- 0.0224)\n",
            "Training Time: 0.01 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Random Forest\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 1.0000\n",
            "Recall: 0.9500\n",
            "F1-Score: 0.9744\n",
            "ROC-AUC: 0.9987\n",
            "CV Accuracy: 0.9667 (+/- 0.0167)\n",
            "Training Time: 0.23 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Extra Trees\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9667\n",
            "Precision: 1.0000\n",
            "Recall: 0.9333\n",
            "F1-Score: 0.9655\n",
            "ROC-AUC: 0.9978\n",
            "CV Accuracy: 0.9542 (+/- 0.0276)\n",
            "Training Time: 0.125 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gradient Boosting\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9833\n",
            "Precision: 1.0000\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9831\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9667 (+/- 0.0191)\n",
            "Training Time: 0.446 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: KNN\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n",
            "ROC-AUC: 1.0000\n",
            "CV Accuracy: 0.9771 (+/- 0.0138)\n",
            "Training Time: 0.002 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (Linear)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9672\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9752\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9812 (+/- 0.0138)\n",
            "Training Time: 0.016 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9833\n",
            "Precision: 0.9833\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9833\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9729 (+/- 0.0106)\n",
            "Training Time: 0.019 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gaussian NB\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9417\n",
            "Precision: 0.9818\n",
            "Recall: 0.9000\n",
            "F1-Score: 0.9391\n",
            "ROC-AUC: 0.9844\n",
            "CV Accuracy: 0.9250 (+/- 0.0369)\n",
            "Training Time: 0.002 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9917\n",
            "Precision: 1.0000\n",
            "Recall: 0.9833\n",
            "F1-Score: 0.9916\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9688 (+/- 0.0174)\n",
            "Training Time: 0.184 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: LightGBM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9831\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9748\n",
            "ROC-AUC: 0.9994\n",
            "CV Accuracy: 0.9750 (+/- 0.0141)\n",
            "Training Time: 0.082 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: CatBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9831\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9748\n",
            "ROC-AUC: 0.9992\n",
            "CV Accuracy: 0.9708 (+/- 0.0167)\n",
            "Training Time: 3.696 seconds\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING COMPLETE - RESULTS SUMMARY\n",
            "================================================================================\n",
            "              Model  Train Time (s)  Accuracy  Precision   Recall  F1-Score  ROC-AUC  CV Mean   CV Std\n",
            "                KNN           0.002  1.000000   1.000000 1.000000  1.000000 1.000000 0.977083 0.013819\n",
            "            XGBoost           0.184  0.991667   1.000000 0.983333  0.991597 0.999444 0.968750 0.017430\n",
            "          SVM (RBF)           0.019  0.983333   0.983333 0.983333  0.983333 0.999444 0.972917 0.010623\n",
            "  Gradient Boosting           0.446  0.983333   1.000000 0.966667  0.983051 0.999444 0.966667 0.019094\n",
            "Logistic Regression           0.006  0.975000   0.967213 0.983333  0.975207 0.999167 0.979167 0.006588\n",
            "   Ridge Classifier           0.004  0.975000   0.967213 0.983333  0.975207      NaN 0.977083 0.007795\n",
            "       SVM (Linear)           0.016  0.975000   0.967213 0.983333  0.975207 0.999167 0.981250 0.013819\n",
            "           LightGBM           0.082  0.975000   0.983051 0.966667  0.974790 0.999444 0.975000 0.014130\n",
            "           CatBoost           3.696  0.975000   0.983051 0.966667  0.974790 0.999167 0.970833 0.016667\n",
            "      Random Forest           0.230  0.975000   1.000000 0.950000  0.974359 0.998750 0.966667 0.016667\n",
            "        Extra Trees           0.125  0.966667   1.000000 0.933333  0.965517 0.997778 0.954167 0.027639\n",
            "     SGD Classifier           0.003  0.958333   0.923077 1.000000  0.960000      NaN 0.958333 0.021850\n",
            "      Decision Tree           0.010  0.958333   0.966102 0.950000  0.957983 0.958333 0.952083 0.022438\n",
            "        Gaussian NB           0.002  0.941667   0.981818 0.900000  0.939130 0.984444 0.925000 0.036917\n",
            "\n",
            "âœ“ Results saved to: ml_classification_results/model_comparison_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "================================================================================\n",
            "âœ“ Comparison plot saved to: ml_classification_results/model_comparison.png\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_all.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_all.png\n",
            "\n",
            "================================================================================\n",
            "TOP 10 MODELS FOR HYPERPARAMETER TUNING:\n",
            "  1. KNN\n",
            "  2. XGBoost\n",
            "  3. SVM (RBF)\n",
            "  4. Gradient Boosting\n",
            "  5. Logistic Regression\n",
            "  6. Ridge Classifier\n",
            "  7. SVM (Linear)\n",
            "  8. LightGBM\n",
            "  9. CatBoost\n",
            "  10. Random Forest\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER TUNING - TOP 10 MODELS\n",
            "================================================================================\n",
            "\n",
            "No parameter grid defined for KNN, skipping tuning\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.3}\n",
            "Best CV F1-Score: 0.9750\n",
            "Tuning time: 9.13 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Best parameters: {'gamma': 0.01, 'C': 10}\n",
            "Best CV F1-Score: 0.9771\n",
            "Tuning time: 1.24 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Gradient Boosting\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "\n",
            "Best parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.3}\n",
            "Best CV F1-Score: 0.9708\n",
            "Tuning time: 45.79 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Logistic Regression\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
            "Best CV F1-Score: 0.9813\n",
            "Tuning time: 0.40 seconds\n",
            "\n",
            "No parameter grid defined for Ridge Classifier, skipping tuning\n",
            "\n",
            "No parameter grid defined for SVM (Linear), skipping tuning\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: LightGBM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'num_leaves': 31, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1}\n",
            "Best CV F1-Score: 0.9772\n",
            "Tuning time: 6.82 seconds\n",
            "\n",
            "No parameter grid defined for CatBoost, skipping tuning\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Random Forest\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
            "Best CV F1-Score: 0.9667\n",
            "Tuning time: 38.17 seconds\n",
            "\n",
            "âœ“ Tuning results saved to: ml_classification_results/top_models_tuning_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "FINAL EVALUATION OF TUNED MODELS\n",
            "================================================================================\n",
            "\n",
            "Evaluating: KNN\n",
            "  Accuracy: 1.0000\n",
            "  F1-Score: 1.0000\n",
            "  ROC-AUC: 1.0000\n",
            "\n",
            "Evaluating: XGBoost\n",
            "  Accuracy: 0.9667\n",
            "  F1-Score: 0.9661\n",
            "  ROC-AUC: 0.9992\n",
            "\n",
            "Evaluating: SVM (RBF)\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9752\n",
            "  ROC-AUC: 0.9994\n",
            "\n",
            "Evaluating: Gradient Boosting\n",
            "  Accuracy: 0.9667\n",
            "  F1-Score: 0.9661\n",
            "  ROC-AUC: 0.9975\n",
            "\n",
            "Evaluating: Logistic Regression\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9752\n",
            "  ROC-AUC: 0.9992\n",
            "\n",
            "Evaluating: Ridge Classifier\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9752\n",
            "  ROC-AUC: nan\n",
            "\n",
            "Evaluating: SVM (Linear)\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9752\n",
            "  ROC-AUC: 0.9992\n",
            "\n",
            "Evaluating: LightGBM\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9748\n",
            "  ROC-AUC: 0.9989\n",
            "\n",
            "Evaluating: CatBoost\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9748\n",
            "  ROC-AUC: 0.9992\n",
            "\n",
            "Evaluating: Random Forest\n",
            "  Accuracy: 0.9750\n",
            "  F1-Score: 0.9744\n",
            "  ROC-AUC: 0.9986\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC\n",
            "                KNN  1.000000   1.000000 1.000000  1.000000 1.000000\n",
            "          SVM (RBF)  0.975000   0.967213 0.983333  0.975207 0.999444\n",
            "   Ridge Classifier  0.975000   0.967213 0.983333  0.975207      NaN\n",
            "Logistic Regression  0.975000   0.967213 0.983333  0.975207 0.999167\n",
            "       SVM (Linear)  0.975000   0.967213 0.983333  0.975207 0.999167\n",
            "           CatBoost  0.975000   0.983051 0.966667  0.974790 0.999167\n",
            "           LightGBM  0.975000   0.983051 0.966667  0.974790 0.998889\n",
            "      Random Forest  0.975000   1.000000 0.950000  0.974359 0.998611\n",
            "            XGBoost  0.966667   0.982759 0.950000  0.966102 0.999167\n",
            "  Gradient Boosting  0.966667   0.982759 0.950000  0.966102 0.997500\n",
            "\n",
            "âœ“ Final results saved to: ml_classification_results/final_tuned_models_results.xlsx\n",
            "\n",
            "ðŸ† BEST MODEL: KNN\n",
            "   F1-Score: 1.0000\n",
            "   Accuracy: 1.0000\n",
            "\n",
            "âœ“ Best model saved to: ml_classification_results/best_model_KNN.pkl\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_tuned.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_tuned.png\n",
            "Feature importance not available for KNN\n",
            "\n",
            "================================================================================\n",
            "DETAILED CLASSIFICATION REPORT - KNN\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      1.00      1.00        60\n",
            "       Human       1.00      1.00      1.00        60\n",
            "\n",
            "    accuracy                           1.00       120\n",
            "   macro avg       1.00      1.00      1.00       120\n",
            "weighted avg       1.00      1.00      1.00       120\n",
            "\n",
            "\n",
            "âœ“ Classification report saved to: ml_classification_results/classification_report_KNN.xlsx\n",
            "âœ“ Scaler saved to: ml_classification_results/scaler.pkl\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\n",
            "================================================================================\n",
            "ðŸ“ All results saved to: ml_classification_results/\n",
            "ðŸ† Best Model: KNN\n",
            "ðŸ“Š F1-Score: 1.0000\n",
            "ðŸ“Š Accuracy: 1.0000\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bengali"
      ],
      "metadata": {
        "id": "O9zoU0iAb3Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              GradientBoostingClassifier, VotingClassifier, StackingClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "\n",
        "# Advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"Warning: XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"Warning: LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"Warning: CatBoost not available\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# FILE PATHS - UPDATE THESE WITH YOUR ACTUAL FILE PATHS\n",
        "FILE_MAPPING = {\n",
        "    'simple_bnlemma': {\n",
        "        'ai': 'AI_simple_bnlemma_PMI.xlsx',\n",
        "        'human': 'human_simple_bnlemma_PMI.xlsx'\n",
        "    },\n",
        "    'adv_bnlp': {\n",
        "        'ai': 'AI_advanced_bnlp_PMI.xlsx',\n",
        "        'human': 'human_advanced_bnlp_PMI.xlsx'\n",
        "    },\n",
        "    'adv_bnltk': {\n",
        "        'ai': 'AI_advanced_bnltk_PMI.xlsx',\n",
        "        'human': 'human_advanced_bnltk_PMI.xlsx'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Statistical test results file\n",
        "BEST_FEATURES_FILE = 'best_features_summary.xlsx'\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = 'ml_classification_results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_SEED = 42\n",
        "CV_FOLDS = 5\n",
        "TOP_N_MODELS_TO_TUNE = 10\n",
        "\n",
        "# Class labels (documented clearly)\n",
        "# 0 = AI-generated text\n",
        "# 1 = Human-written text\n",
        "AI_LABEL = 0\n",
        "HUMAN_LABEL = 1\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Class Labels: AI = {AI_LABEL}, Human = {HUMAN_LABEL}\")\n",
        "print(f\"Test Size: {TEST_SIZE}, CV Folds: {CV_FOLDS}, Random Seed: {RANDOM_SEED}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SOURCE NAME MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "def map_source_name(source_str):\n",
        "    \"\"\"Map source names from Excel to file mapping keys\"\"\"\n",
        "    mapping = {\n",
        "        'bnlemma': 'simple_bnlemma',\n",
        "        'bnlp': 'adv_bnlp',\n",
        "        'bnltk': 'adv_bnltk'\n",
        "    }\n",
        "    return mapping.get(source_str, source_str.lower().replace(' ', '_'))\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def load_feature_recommendations(filepath):\n",
        "    \"\"\"Load and filter highly recommended features with their sources\"\"\"\n",
        "    print(f\"Loading feature recommendations from: {filepath}\")\n",
        "\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Filter only highly recommended features\n",
        "    highly_recommended = df[df['feature_recommendation'] == 'Highly Recommended'].copy()\n",
        "\n",
        "    print(f\"Total features in file: {len(df)}\")\n",
        "    print(f\"Highly recommended features: {len(highly_recommended)}\")\n",
        "\n",
        "    # Create feature-source mapping\n",
        "    feature_source_map = {}\n",
        "    for _, row in highly_recommended.iterrows():\n",
        "        feature_name = row['metric']\n",
        "        source = map_source_name(row['source'])\n",
        "        feature_source_map[feature_name] = source\n",
        "\n",
        "    print(f\"\\nHighly Recommended Features and Their Sources:\")\n",
        "    for feature, source in feature_source_map.items():\n",
        "        print(f\"  - {feature}: {source}\")\n",
        "\n",
        "    return feature_source_map\n",
        "\n",
        "def load_excel_data(filepath, max_rows=300):\n",
        "    \"\"\"Load Excel file and clean it\"\"\"\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Remove 'Average' row if exists\n",
        "    df = df[df['Graph'] != 'Average'].copy()\n",
        "\n",
        "    # Remove 'Graph' column\n",
        "    if 'Graph' in df.columns:\n",
        "        df = df.drop('Graph', axis=1)\n",
        "\n",
        "    # Keep only first max_rows\n",
        "    df = df.head(max_rows)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_mixed_source_dataset(feature_source_map, file_mapping):\n",
        "    \"\"\"\n",
        "    Build dataset by selecting each feature from its best-performing source\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BUILDING MIXED-SOURCE DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    ai_features = []\n",
        "    human_features = []\n",
        "\n",
        "    for feature_name, source in feature_source_map.items():\n",
        "        print(f\"\\nProcessing feature: {feature_name} from source: {source}\")\n",
        "\n",
        "        try:\n",
        "            # Load AI data\n",
        "            ai_file = file_mapping[source]['ai']\n",
        "            ai_df = load_excel_data(ai_file)\n",
        "\n",
        "            if feature_name not in ai_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {ai_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            ai_feature_data = ai_df[[feature_name]].copy()\n",
        "            ai_feature_data.columns = [feature_name]\n",
        "\n",
        "            # Load Human data\n",
        "            human_file = file_mapping[source]['human']\n",
        "            human_df = load_excel_data(human_file)\n",
        "\n",
        "            if feature_name not in human_df.columns:\n",
        "                print(f\"  Warning: '{feature_name}' not found in {human_file}, skipping\")\n",
        "                continue\n",
        "\n",
        "            human_feature_data = human_df[[feature_name]].copy()\n",
        "            human_feature_data.columns = [feature_name]\n",
        "\n",
        "            ai_features.append(ai_feature_data)\n",
        "            human_features.append(human_feature_data)\n",
        "\n",
        "            print(f\"  âœ“ Successfully loaded from both AI and Human files\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {feature_name} from {source}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Combine all features\n",
        "    print(\"\\nCombining features...\")\n",
        "    ai_combined = pd.concat(ai_features, axis=1)\n",
        "    human_combined = pd.concat(human_features, axis=1)\n",
        "\n",
        "    # Add labels\n",
        "    ai_combined['label'] = AI_LABEL\n",
        "    human_combined['label'] = HUMAN_LABEL\n",
        "\n",
        "    # Combine AI and Human\n",
        "    final_df = pd.concat([ai_combined, human_combined], axis=0, ignore_index=True)\n",
        "\n",
        "    print(f\"\\nâœ“ Dataset built successfully!\")\n",
        "    print(f\"  Shape: {final_df.shape}\")\n",
        "    print(f\"  Features: {final_df.shape[1] - 1}\")\n",
        "    print(f\"  Samples: {final_df.shape[0]}\")\n",
        "    print(f\"  AI samples: {(final_df['label'] == AI_LABEL).sum()}\")\n",
        "    print(f\"  Human samples: {(final_df['label'] == HUMAN_LABEL).sum()}\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ============================================================================\n",
        "# PREPROCESSING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Handle missing values and prepare data\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('label', axis=1)\n",
        "    y = df['label']\n",
        "\n",
        "    print(f\"Initial shape: {X.shape}\")\n",
        "\n",
        "    # Handle -1 values (replace with column median or 0)\n",
        "    print(\"\\nHandling missing values (-1)...\")\n",
        "    for col in X.columns:\n",
        "        missing_count = (X[col] == -1).sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values\")\n",
        "            # Replace -1 with median (or 0 if all are -1)\n",
        "            median_val = X[X[col] != -1][col].median()\n",
        "            if pd.isna(median_val):\n",
        "                X[col] = X[col].replace(-1, 0)\n",
        "            else:\n",
        "                X[col] = X[col].replace(-1, median_val)\n",
        "\n",
        "    # Handle boolean columns (convert True/False to 1/0)\n",
        "    bool_cols = X.select_dtypes(include=['bool']).columns\n",
        "    if len(bool_cols) > 0:\n",
        "        print(f\"\\nConverting boolean columns to numeric: {list(bool_cols)}\")\n",
        "        X[bool_cols] = X[bool_cols].astype(int)\n",
        "\n",
        "    # Check for any remaining NaN or inf values\n",
        "    if X.isnull().any().any():\n",
        "        print(\"\\nWarning: NaN values found, filling with 0\")\n",
        "        X = X.fillna(0)\n",
        "\n",
        "    if np.isinf(X.values).any():\n",
        "        print(\"\\nWarning: Infinite values found, replacing with column max\")\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "        X = X.fillna(X.max())\n",
        "\n",
        "    print(f\"\\nâœ“ Preprocessing complete!\")\n",
        "    print(f\"  Final shape: {X.shape}\")\n",
        "    print(f\"  Class distribution:\")\n",
        "    print(f\"    AI (0): {(y == 0).sum()}\")\n",
        "    print(f\"    Human (1): {(y == 1).sum()}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def split_and_scale_data(X, y):\n",
        "    \"\"\"Split data and scale features\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SPLITTING AND SCALING DATA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "\n",
        "    print(\"âœ“ Scaling complete (StandardScaler)\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "def get_all_models():\n",
        "    \"\"\"Return dictionary of all models to benchmark\"\"\"\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Ridge Classifier': RidgeClassifier(random_state=RANDOM_SEED),\n",
        "        'SGD Classifier': SGDClassifier(random_state=RANDOM_SEED, max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "        'Random Forest': RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_SEED, n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=RANDOM_SEED),\n",
        "        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=RANDOM_SEED),\n",
        "        'Gaussian NB': GaussianNB(),\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models['XGBoost'] = xgb.XGBClassifier(random_state=RANDOM_SEED, eval_metric='logloss')\n",
        "\n",
        "    # Add LightGBM if available\n",
        "    if LIGHTGBM_AVAILABLE:\n",
        "        models['LightGBM'] = lgb.LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)\n",
        "\n",
        "    # Add CatBoost if available\n",
        "    if CATBOOST_AVAILABLE:\n",
        "        models['CatBoost'] = cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=0)\n",
        "\n",
        "    return models\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_and_evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate a single model\"\"\"\n",
        "    print(f\"\\n{'â”€' * 80}\")\n",
        "    print(f\"Training: {name}\")\n",
        "    print(f\"{'â”€' * 80}\")\n",
        "\n",
        "    results = {'Model': name}\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    results['Train Time (s)'] = round(train_time, 3)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Metrics\n",
        "    results['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results['Precision'] = precision_score(y_test, y_pred)\n",
        "    results['Recall'] = recall_score(y_test, y_pred)\n",
        "    results['F1-Score'] = f1_score(y_test, y_pred)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        results['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        results['ROC-AUC'] = np.nan\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')\n",
        "    results['CV Mean'] = cv_scores.mean()\n",
        "    results['CV Std'] = cv_scores.std()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Accuracy: {results['Accuracy']:.4f}\")\n",
        "    print(f\"Precision: {results['Precision']:.4f}\")\n",
        "    print(f\"Recall: {results['Recall']:.4f}\")\n",
        "    print(f\"F1-Score: {results['F1-Score']:.4f}\")\n",
        "    print(f\"ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "    print(f\"CV Accuracy: {results['CV Mean']:.4f} (+/- {results['CV Std']:.4f})\")\n",
        "    print(f\"Training Time: {results['Train Time (s)']} seconds\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return results, model, y_pred, y_pred_proba, cm\n",
        "\n",
        "def benchmark_all_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate all models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING ALL MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    models = get_all_models()\n",
        "    results_list = []\n",
        "    trained_models = {}\n",
        "    predictions = {}\n",
        "    confusion_matrices = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            results, trained_model, y_pred, y_pred_proba, cm = train_and_evaluate_model(\n",
        "                name, model, X_train, X_test, y_train, y_test\n",
        "            )\n",
        "            results_list.append(results)\n",
        "            trained_models[name] = trained_model\n",
        "            predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "            confusion_matrices[name] = cm\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARKING COMPLETE - RESULTS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    results_file = os.path.join(OUTPUT_DIR, 'model_comparison_results.xlsx')\n",
        "    results_df.to_excel(results_file, index=False)\n",
        "    print(f\"\\nâœ“ Results saved to: {results_file}\")\n",
        "\n",
        "    return results_df, trained_models, predictions, confusion_matrices\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING\n",
        "# ============================================================================\n",
        "\n",
        "def get_param_grids():\n",
        "    \"\"\"Define parameter grids for hyperparameter tuning\"\"\"\n",
        "    param_grids = {\n",
        "        'Random Forest': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'num_leaves': [31, 50, 70]\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs', 'liblinear']\n",
        "        },\n",
        "        'SVM (RBF)': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return param_grids\n",
        "\n",
        "def tune_top_models(top_model_names, trained_models, X_train, y_train):\n",
        "    \"\"\"Perform hyperparameter tuning on top models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"HYPERPARAMETER TUNING - TOP {len(top_model_names)} MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    param_grids = get_param_grids()\n",
        "    tuned_models = {}\n",
        "    tuning_results = []\n",
        "\n",
        "    for name in top_model_names:\n",
        "        if name not in param_grids:\n",
        "            print(f\"\\nNo parameter grid defined for {name}, skipping tuning\")\n",
        "            tuned_models[name] = trained_models[name]\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'â”€' * 80}\")\n",
        "        print(f\"Tuning: {name}\")\n",
        "        print(f\"{'â”€' * 80}\")\n",
        "\n",
        "        base_model = trained_models[name]\n",
        "        param_grid = param_grids[name]\n",
        "\n",
        "        # Use RandomizedSearchCV for faster tuning\n",
        "        search = RandomizedSearchCV(\n",
        "            base_model, param_grid, n_iter=20, cv=CV_FOLDS,\n",
        "            scoring='f1', random_state=RANDOM_SEED, n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        search.fit(X_train, y_train)\n",
        "        tune_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nBest parameters: {search.best_params_}\")\n",
        "        print(f\"Best CV F1-Score: {search.best_score_:.4f}\")\n",
        "        print(f\"Tuning time: {tune_time:.2f} seconds\")\n",
        "\n",
        "        tuned_models[name] = search.best_estimator_\n",
        "\n",
        "        tuning_results.append({\n",
        "            'Model': name,\n",
        "            'Best CV F1-Score': search.best_score_,\n",
        "            'Best Parameters': str(search.best_params_),\n",
        "            'Tuning Time (s)': round(tune_time, 2)\n",
        "        })\n",
        "\n",
        "    # Save tuning results\n",
        "    tuning_df = pd.DataFrame(tuning_results)\n",
        "    tuning_file = os.path.join(OUTPUT_DIR, 'top_models_tuning_results.xlsx')\n",
        "    tuning_df.to_excel(tuning_file, index=False)\n",
        "    print(f\"\\nâœ“ Tuning results saved to: {tuning_file}\")\n",
        "\n",
        "    return tuned_models\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def final_evaluation(tuned_models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Evaluate tuned models on test set\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL EVALUATION OF TUNED MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    final_results = []\n",
        "    final_predictions = {}\n",
        "    final_cms = {}\n",
        "\n",
        "    for name, model in tuned_models.items():\n",
        "        print(f\"\\nEvaluating: {name}\")\n",
        "\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Metrics\n",
        "        results = {\n",
        "            'Model': name,\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Precision': precision_score(y_test, y_pred),\n",
        "            'Recall': recall_score(y_test, y_pred),\n",
        "            'F1-Score': f1_score(y_test, y_pred),\n",
        "            'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else np.nan\n",
        "        }\n",
        "\n",
        "        final_results.append(results)\n",
        "        final_predictions[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
        "        final_cms[name] = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Accuracy: {results['Accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score: {results['F1-Score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {results['ROC-AUC']:.4f}\")\n",
        "\n",
        "    final_df = pd.DataFrame(final_results).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(final_df.to_string(index=False))\n",
        "\n",
        "    # Save final results\n",
        "    final_file = os.path.join(OUTPUT_DIR, 'final_tuned_models_results.xlsx')\n",
        "    final_df.to_excel(final_file, index=False)\n",
        "    print(f\"\\nâœ“ Final results saved to: {final_file}\")\n",
        "\n",
        "    # Get best model\n",
        "    best_model_name = final_df.iloc[0]['Model']\n",
        "    best_model = tuned_models[best_model_name]\n",
        "\n",
        "    print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"   Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    model_file = os.path.join(OUTPUT_DIR, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
        "    joblib.dump(best_model, model_file)\n",
        "    print(f\"\\nâœ“ Best model saved to: {model_file}\")\n",
        "\n",
        "    return final_df, best_model, best_model_name, final_predictions, final_cms\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_model_comparison(results_df, save_path):\n",
        "    \"\"\"Plot model performance comparison\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        data = results_df.sort_values(metric, ascending=True)\n",
        "\n",
        "        ax.barh(data['Model'], data[metric], color='steelblue')\n",
        "        ax.set_xlabel(metric, fontsize=12)\n",
        "        ax.set_title(f'{metric} by Model', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlim([0, 1])\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(data[metric]):\n",
        "            ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Comparison plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrices(confusion_matrices, save_path):\n",
        "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "    n_models = len(confusion_matrices)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_models > 1 else [axes]\n",
        "\n",
        "    for idx, (name, cm) in enumerate(confusion_matrices.items()):\n",
        "        ax = axes[idx]\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                   xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
        "        ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_models, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Confusion matrices saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curves(predictions, y_test, save_path):\n",
        "    \"\"\"Plot ROC curves for all models\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for name, preds in predictions.items():\n",
        "        if preds['y_pred_proba'] is not None:\n",
        "            fpr, tpr, _ = roc_curve(y_test, preds['y_pred_proba'])\n",
        "            auc = roc_auc_score(y_test, preds['y_pred_proba'])\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='lower right', fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ ROC curves saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(model, feature_names, model_name, save_path):\n",
        "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "    if not hasattr(model, 'feature_importances_'):\n",
        "        print(f\"Feature importance not available for {model_name}\")\n",
        "        return\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:20]  # Top 20\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(len(indices)), importances[indices], color='steelblue')\n",
        "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance', fontsize=12)\n",
        "    plt.title(f'Top 20 Feature Importance - {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Feature importance plot saved to: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save to Excel\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    excel_path = save_path.replace('.png', '.xlsx')\n",
        "    importance_df.to_excel(excel_path, index=False)\n",
        "    print(f\"âœ“ Feature importance data saved to: {excel_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    # Step 1: Load feature recommendations\n",
        "    feature_source_map = load_feature_recommendations(BEST_FEATURES_FILE)\n",
        "\n",
        "    # Step 2: Build mixed-source dataset\n",
        "    df = build_mixed_source_dataset(feature_source_map, FILE_MAPPING)\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE FINAL COMBINED DATASET - ADD THIS CODE HERE\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAVING FINAL COMBINED DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Save the complete dataset (before preprocessing)\n",
        "    combined_dataset_file = os.path.join(OUTPUT_DIR, 'final_combined_dataset.xlsx')\n",
        "    df.to_excel(combined_dataset_file, index=False)\n",
        "    print(f\"âœ“ Final combined dataset saved to: {combined_dataset_file}\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Also save as CSV for easier access\n",
        "    combined_dataset_csv = os.path.join(OUTPUT_DIR, 'final_combined_dataset.csv')\n",
        "    df.to_csv(combined_dataset_csv, index=False)\n",
        "    print(f\"âœ“ Final combined dataset (CSV) saved to: {combined_dataset_csv}\")\n",
        "\n",
        "    # Save a summary of data sources for each feature\n",
        "    feature_source_summary = pd.DataFrame([\n",
        "        {'Feature': feature, 'Source': source}\n",
        "        for feature, source in feature_source_map.items()\n",
        "    ])\n",
        "    source_summary_file = os.path.join(OUTPUT_DIR, 'feature_source_mapping.xlsx')\n",
        "    feature_source_summary.to_excel(source_summary_file, index=False)\n",
        "    print(f\"âœ“ Feature-source mapping saved to: {source_summary_file}\")\n",
        "    print(\"=\" * 80)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Step 3: Preprocess data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Split and scale\n",
        "    X_train, X_test, y_train, y_test, scaler = split_and_scale_data(X, y)\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE PREPROCESSED DATASETS - ADD THIS CODE HERE (OPTIONAL)\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAVING PREPROCESSED DATASETS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Save train and test sets (before scaling)\n",
        "    train_df = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_df = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    train_file = os.path.join(OUTPUT_DIR, 'train_set.xlsx')\n",
        "    test_file = os.path.join(OUTPUT_DIR, 'test_set.xlsx')\n",
        "\n",
        "    train_df.to_excel(train_file, index=False)\n",
        "    test_df.to_excel(test_file, index=False)\n",
        "\n",
        "    print(f\"âœ“ Training set saved to: {train_file}\")\n",
        "    print(f\"  Shape: {train_df.shape}\")\n",
        "    print(f\"âœ“ Test set saved to: {test_file}\")\n",
        "    print(f\"  Shape: {test_df.shape}\")\n",
        "    print(\"=\" * 80)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Step 5: Benchmark all models\n",
        "    results_df, trained_models, predictions, confusion_matrices = benchmark_all_models(\n",
        "        X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 6: Visualizations for all models\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    plot_model_comparison(\n",
        "        results_df,\n",
        "        os.path.join(OUTPUT_DIR, 'model_comparison.png')\n",
        "    )\n",
        "\n",
        "    plot_confusion_matrices(\n",
        "        confusion_matrices,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_all.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        predictions,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_all.png')\n",
        "    )\n",
        "\n",
        "    # Step 7: Select top models for tuning\n",
        "    top_models = results_df.head(TOP_N_MODELS_TO_TUNE)['Model'].tolist()\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(f\"TOP {TOP_N_MODELS_TO_TUNE} MODELS FOR HYPERPARAMETER TUNING:\")\n",
        "    for i, model_name in enumerate(top_models, 1):\n",
        "        print(f\"  {i}. {model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 8: Hyperparameter tuning\n",
        "    tuned_models = tune_top_models(top_models, trained_models, X_train, y_train)\n",
        "\n",
        "    # Step 9: Final evaluation\n",
        "    final_df, best_model, best_model_name, final_preds, final_cms = final_evaluation(\n",
        "        tuned_models, X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Step 10: Final visualizations\n",
        "    plot_confusion_matrices(\n",
        "        final_cms,\n",
        "        os.path.join(OUTPUT_DIR, 'confusion_matrices_tuned.png')\n",
        "    )\n",
        "\n",
        "    plot_roc_curves(\n",
        "        final_preds,\n",
        "        y_test,\n",
        "        os.path.join(OUTPUT_DIR, 'roc_curves_tuned.png')\n",
        "    )\n",
        "\n",
        "    # Step 11: Feature importance for best model\n",
        "    plot_feature_importance(\n",
        "        best_model,\n",
        "        X.columns.tolist(),\n",
        "        best_model_name,\n",
        "        os.path.join(OUTPUT_DIR, f'feature_importance_{best_model_name.replace(\" \", \"_\")}.png')\n",
        "    )\n",
        "\n",
        "    # Step 12: Generate classification report for best model\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred_best, target_names=['AI', 'Human']))\n",
        "\n",
        "    # Save classification report\n",
        "    report_dict = classification_report(y_test, y_pred_best, target_names=['AI', 'Human'], output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    report_file = os.path.join(OUTPUT_DIR, f'classification_report_{best_model_name.replace(\" \", \"_\")}.xlsx')\n",
        "    report_df.to_excel(report_file)\n",
        "    print(f\"\\nâœ“ Classification report saved to: {report_file}\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_file = os.path.join(OUTPUT_DIR, 'scaler.pkl')\n",
        "    joblib.dump(scaler, scaler_file)\n",
        "    print(f\"âœ“ Scaler saved to: {scaler_file}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“ All results saved to: {OUTPUT_DIR}/\")\n",
        "    print(f\"ðŸ† Best Model: {best_model_name}\")\n",
        "    print(f\"ðŸ“Š F1-Score: {final_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"ðŸ“Š Accuracy: {final_df.iloc[0]['Accuracy']:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6jxdQzQb2wK",
        "outputId": "4d566a61-5240-49d6-8a0a-59a2faaa9205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AI vs HUMAN TEXT CLASSIFICATION - ML BENCHMARKING SYSTEM\n",
            "================================================================================\n",
            "Class Labels: AI = 0, Human = 1\n",
            "Test Size: 0.2, CV Folds: 5, Random Seed: 42\n",
            "================================================================================\n",
            "\n",
            "Loading feature recommendations from: best_features_summary.xlsx\n",
            "Total features in file: 21\n",
            "Highly recommended features: 12\n",
            "\n",
            "Highly Recommended Features and Their Sources:\n",
            "  - algebraic_connectivity: adv_bnlp\n",
            "  - avg_shortest_path_length: adv_bnlp\n",
            "  - avg_closeness_centrality: adv_bnlp\n",
            "  - transitivity: adv_bnlp\n",
            "  - avg_betweenness_centrality: adv_bnlp\n",
            "  - diameter: simple_bnlemma\n",
            "  - radius: simple_bnlemma\n",
            "  - top_eigenvalue: adv_bnlp\n",
            "  - avg_clustering: adv_bnlp\n",
            "  - wiener_index: simple_bnlemma\n",
            "  - avg_degree_centrality: simple_bnlemma\n",
            "  - density: simple_bnlemma\n",
            "\n",
            "================================================================================\n",
            "BUILDING MIXED-SOURCE DATASET\n",
            "================================================================================\n",
            "\n",
            "Processing feature: algebraic_connectivity from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_shortest_path_length from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_closeness_centrality from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: transitivity from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_betweenness_centrality from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: diameter from source: simple_bnlemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: radius from source: simple_bnlemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: top_eigenvalue from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_clustering from source: adv_bnlp\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: wiener_index from source: simple_bnlemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: avg_degree_centrality from source: simple_bnlemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Processing feature: density from source: simple_bnlemma\n",
            "  âœ“ Successfully loaded from both AI and Human files\n",
            "\n",
            "Combining features...\n",
            "\n",
            "âœ“ Dataset built successfully!\n",
            "  Shape: (600, 13)\n",
            "  Features: 12\n",
            "  Samples: 600\n",
            "  AI samples: 300\n",
            "  Human samples: 300\n",
            "\n",
            "================================================================================\n",
            "SAVING FINAL COMBINED DATASET\n",
            "================================================================================\n",
            "âœ“ Final combined dataset saved to: ml_classification_results/final_combined_dataset.xlsx\n",
            "  Shape: (600, 13)\n",
            "  Columns: ['algebraic_connectivity', 'avg_shortest_path_length', 'avg_closeness_centrality', 'transitivity', 'avg_betweenness_centrality', 'diameter', 'radius', 'top_eigenvalue', 'avg_clustering', 'wiener_index', 'avg_degree_centrality', 'density', 'label']\n",
            "âœ“ Final combined dataset (CSV) saved to: ml_classification_results/final_combined_dataset.csv\n",
            "âœ“ Feature-source mapping saved to: ml_classification_results/feature_source_mapping.xlsx\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING DATA\n",
            "================================================================================\n",
            "Initial shape: (600, 12)\n",
            "\n",
            "Handling missing values (-1)...\n",
            "\n",
            "âœ“ Preprocessing complete!\n",
            "  Final shape: (600, 12)\n",
            "  Class distribution:\n",
            "    AI (0): 300\n",
            "    Human (1): 300\n",
            "\n",
            "================================================================================\n",
            "SPLITTING AND SCALING DATA\n",
            "================================================================================\n",
            "Train set: 480 samples\n",
            "Test set: 120 samples\n",
            "âœ“ Scaling complete (StandardScaler)\n",
            "\n",
            "================================================================================\n",
            "SAVING PREPROCESSED DATASETS\n",
            "================================================================================\n",
            "âœ“ Training set saved to: ml_classification_results/train_set.xlsx\n",
            "  Shape: (480, 13)\n",
            "âœ“ Test set saved to: ml_classification_results/test_set.xlsx\n",
            "  Shape: (120, 13)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Logistic Regression\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7750\n",
            "Precision: 0.7538\n",
            "Recall: 0.8167\n",
            "F1-Score: 0.7840\n",
            "ROC-AUC: 0.8636\n",
            "CV Accuracy: 0.8167 (+/- 0.0292)\n",
            "Training Time: 0.025 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Ridge Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7750\n",
            "Precision: 0.7619\n",
            "Recall: 0.8000\n",
            "F1-Score: 0.7805\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.8208 (+/- 0.0319)\n",
            "Training Time: 0.016 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SGD Classifier\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7417\n",
            "Precision: 0.6933\n",
            "Recall: 0.8667\n",
            "F1-Score: 0.7704\n",
            "ROC-AUC: nan\n",
            "CV Accuracy: 0.7771 (+/- 0.0268)\n",
            "Training Time: 0.004 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Decision Tree\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7333\n",
            "Precision: 0.7333\n",
            "Recall: 0.7333\n",
            "F1-Score: 0.7333\n",
            "ROC-AUC: 0.7333\n",
            "CV Accuracy: 0.7396 (+/- 0.0378)\n",
            "Training Time: 0.008 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Random Forest\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7917\n",
            "Precision: 0.7612\n",
            "Recall: 0.8500\n",
            "F1-Score: 0.8031\n",
            "ROC-AUC: 0.8596\n",
            "CV Accuracy: 0.8208 (+/- 0.0319)\n",
            "Training Time: 0.255 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Extra Trees\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7917\n",
            "Precision: 0.7536\n",
            "Recall: 0.8667\n",
            "F1-Score: 0.8062\n",
            "ROC-AUC: 0.8718\n",
            "CV Accuracy: 0.8313 (+/- 0.0424)\n",
            "Training Time: 0.167 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gradient Boosting\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.8000\n",
            "Precision: 0.7903\n",
            "Recall: 0.8167\n",
            "F1-Score: 0.8033\n",
            "ROC-AUC: 0.8656\n",
            "CV Accuracy: 0.8146 (+/- 0.0259)\n",
            "Training Time: 0.416 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: KNN\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7417\n",
            "Precision: 0.7101\n",
            "Recall: 0.8167\n",
            "F1-Score: 0.7597\n",
            "ROC-AUC: 0.8140\n",
            "CV Accuracy: 0.7625 (+/- 0.0275)\n",
            "Training Time: 0.004 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (Linear)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7750\n",
            "Precision: 0.7705\n",
            "Recall: 0.7833\n",
            "F1-Score: 0.7769\n",
            "ROC-AUC: 0.8619\n",
            "CV Accuracy: 0.8250 (+/- 0.0312)\n",
            "Training Time: 0.035 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.8083\n",
            "Precision: 0.7761\n",
            "Recall: 0.8667\n",
            "F1-Score: 0.8189\n",
            "ROC-AUC: 0.8562\n",
            "CV Accuracy: 0.8083 (+/- 0.0276)\n",
            "Training Time: 0.036 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: Gaussian NB\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7500\n",
            "Precision: 0.7885\n",
            "Recall: 0.6833\n",
            "F1-Score: 0.7321\n",
            "ROC-AUC: 0.8264\n",
            "CV Accuracy: 0.7729 (+/- 0.0212)\n",
            "Training Time: 0.002 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.8000\n",
            "Precision: 0.8000\n",
            "Recall: 0.8000\n",
            "F1-Score: 0.8000\n",
            "ROC-AUC: 0.8742\n",
            "CV Accuracy: 0.8271 (+/- 0.0327)\n",
            "Training Time: 0.142 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: LightGBM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.7583\n",
            "Precision: 0.7385\n",
            "Recall: 0.8000\n",
            "F1-Score: 0.7680\n",
            "ROC-AUC: 0.8539\n",
            "CV Accuracy: 0.8313 (+/- 0.0381)\n",
            "Training Time: 0.108 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Training: CatBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy: 0.8083\n",
            "Precision: 0.7846\n",
            "Recall: 0.8500\n",
            "F1-Score: 0.8160\n",
            "ROC-AUC: 0.8631\n",
            "CV Accuracy: 0.8146 (+/- 0.0290)\n",
            "Training Time: 3.365 seconds\n",
            "\n",
            "================================================================================\n",
            "BENCHMARKING COMPLETE - RESULTS SUMMARY\n",
            "================================================================================\n",
            "              Model  Train Time (s)  Accuracy  Precision   Recall  F1-Score  ROC-AUC  CV Mean   CV Std\n",
            "          SVM (RBF)           0.036  0.808333   0.776119 0.866667  0.818898 0.856250 0.808333 0.027639\n",
            "           CatBoost           3.365  0.808333   0.784615 0.850000  0.816000 0.863056 0.814583 0.029017\n",
            "        Extra Trees           0.167  0.791667   0.753623 0.866667  0.806202 0.871806 0.831250 0.042390\n",
            "  Gradient Boosting           0.416  0.800000   0.790323 0.816667  0.803279 0.865556 0.814583 0.025853\n",
            "      Random Forest           0.255  0.791667   0.761194 0.850000  0.803150 0.859583 0.820833 0.031869\n",
            "            XGBoost           0.142  0.800000   0.800000 0.800000  0.800000 0.874167 0.827083 0.032676\n",
            "Logistic Regression           0.025  0.775000   0.753846 0.816667  0.784000 0.863611 0.816667 0.029167\n",
            "   Ridge Classifier           0.016  0.775000   0.761905 0.800000  0.780488      NaN 0.820833 0.031869\n",
            "       SVM (Linear)           0.035  0.775000   0.770492 0.783333  0.776860 0.861944 0.825000 0.031180\n",
            "     SGD Classifier           0.004  0.741667   0.693333 0.866667  0.770370      NaN 0.777083 0.026842\n",
            "           LightGBM           0.108  0.758333   0.738462 0.800000  0.768000 0.853889 0.831250 0.038074\n",
            "                KNN           0.004  0.741667   0.710145 0.816667  0.759690 0.814028 0.762500 0.027481\n",
            "      Decision Tree           0.008  0.733333   0.733333 0.733333  0.733333 0.733333 0.739583 0.037846\n",
            "        Gaussian NB           0.002  0.750000   0.788462 0.683333  0.732143 0.826389 0.772917 0.021246\n",
            "\n",
            "âœ“ Results saved to: ml_classification_results/model_comparison_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "================================================================================\n",
            "âœ“ Comparison plot saved to: ml_classification_results/model_comparison.png\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_all.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_all.png\n",
            "\n",
            "================================================================================\n",
            "TOP 10 MODELS FOR HYPERPARAMETER TUNING:\n",
            "  1. SVM (RBF)\n",
            "  2. CatBoost\n",
            "  3. Extra Trees\n",
            "  4. Gradient Boosting\n",
            "  5. Random Forest\n",
            "  6. XGBoost\n",
            "  7. Logistic Regression\n",
            "  8. Ridge Classifier\n",
            "  9. SVM (Linear)\n",
            "  10. SGD Classifier\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "HYPERPARAMETER TUNING - TOP 10 MODELS\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: SVM (RBF)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Best parameters: {'gamma': 'scale', 'C': 10}\n",
            "Best CV F1-Score: 0.8381\n",
            "Tuning time: 5.08 seconds\n",
            "\n",
            "No parameter grid defined for CatBoost, skipping tuning\n",
            "\n",
            "No parameter grid defined for Extra Trees, skipping tuning\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Gradient Boosting\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "\n",
            "Best parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.3}\n",
            "Best CV F1-Score: 0.8389\n",
            "Tuning time: 62.37 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Random Forest\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10}\n",
            "Best CV F1-Score: 0.8388\n",
            "Tuning time: 42.20 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: XGBoost\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.3}\n",
            "Best CV F1-Score: 0.8425\n",
            "Tuning time: 15.96 seconds\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Tuning: Logistic Regression\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
            "Best CV F1-Score: 0.8371\n",
            "Tuning time: 0.52 seconds\n",
            "\n",
            "No parameter grid defined for Ridge Classifier, skipping tuning\n",
            "\n",
            "No parameter grid defined for SVM (Linear), skipping tuning\n",
            "\n",
            "No parameter grid defined for SGD Classifier, skipping tuning\n",
            "\n",
            "âœ“ Tuning results saved to: ml_classification_results/top_models_tuning_results.xlsx\n",
            "\n",
            "================================================================================\n",
            "FINAL EVALUATION OF TUNED MODELS\n",
            "================================================================================\n",
            "\n",
            "Evaluating: SVM (RBF)\n",
            "  Accuracy: 0.8083\n",
            "  F1-Score: 0.8160\n",
            "  ROC-AUC: 0.8561\n",
            "\n",
            "Evaluating: CatBoost\n",
            "  Accuracy: 0.8083\n",
            "  F1-Score: 0.8160\n",
            "  ROC-AUC: 0.8631\n",
            "\n",
            "Evaluating: Extra Trees\n",
            "  Accuracy: 0.7917\n",
            "  F1-Score: 0.8062\n",
            "  ROC-AUC: 0.8718\n",
            "\n",
            "Evaluating: Gradient Boosting\n",
            "  Accuracy: 0.7750\n",
            "  F1-Score: 0.7874\n",
            "  ROC-AUC: 0.8619\n",
            "\n",
            "Evaluating: Random Forest\n",
            "  Accuracy: 0.7917\n",
            "  F1-Score: 0.8031\n",
            "  ROC-AUC: 0.8619\n",
            "\n",
            "Evaluating: XGBoost\n",
            "  Accuracy: 0.7833\n",
            "  F1-Score: 0.7869\n",
            "  ROC-AUC: 0.8536\n",
            "\n",
            "Evaluating: Logistic Regression\n",
            "  Accuracy: 0.7750\n",
            "  F1-Score: 0.7769\n",
            "  ROC-AUC: 0.8656\n",
            "\n",
            "Evaluating: Ridge Classifier\n",
            "  Accuracy: 0.7750\n",
            "  F1-Score: 0.7805\n",
            "  ROC-AUC: nan\n",
            "\n",
            "Evaluating: SVM (Linear)\n",
            "  Accuracy: 0.7750\n",
            "  F1-Score: 0.7769\n",
            "  ROC-AUC: 0.8619\n",
            "\n",
            "Evaluating: SGD Classifier\n",
            "  Accuracy: 0.7417\n",
            "  F1-Score: 0.7704\n",
            "  ROC-AUC: nan\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC\n",
            "          SVM (RBF)  0.808333   0.784615 0.850000  0.816000 0.856111\n",
            "           CatBoost  0.808333   0.784615 0.850000  0.816000 0.863056\n",
            "        Extra Trees  0.791667   0.753623 0.866667  0.806202 0.871806\n",
            "      Random Forest  0.791667   0.761194 0.850000  0.803150 0.861944\n",
            "  Gradient Boosting  0.775000   0.746269 0.833333  0.787402 0.861944\n",
            "            XGBoost  0.783333   0.774194 0.800000  0.786885 0.853611\n",
            "   Ridge Classifier  0.775000   0.761905 0.800000  0.780488      NaN\n",
            "Logistic Regression  0.775000   0.770492 0.783333  0.776860 0.865556\n",
            "       SVM (Linear)  0.775000   0.770492 0.783333  0.776860 0.861944\n",
            "     SGD Classifier  0.741667   0.693333 0.866667  0.770370      NaN\n",
            "\n",
            "âœ“ Final results saved to: ml_classification_results/final_tuned_models_results.xlsx\n",
            "\n",
            "ðŸ† BEST MODEL: SVM (RBF)\n",
            "   F1-Score: 0.8160\n",
            "   Accuracy: 0.8083\n",
            "\n",
            "âœ“ Best model saved to: ml_classification_results/best_model_SVM_(RBF).pkl\n",
            "âœ“ Confusion matrices saved to: ml_classification_results/confusion_matrices_tuned.png\n",
            "âœ“ ROC curves saved to: ml_classification_results/roc_curves_tuned.png\n",
            "Feature importance not available for SVM (RBF)\n",
            "\n",
            "================================================================================\n",
            "DETAILED CLASSIFICATION REPORT - SVM (RBF)\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.84      0.77      0.80        60\n",
            "       Human       0.78      0.85      0.82        60\n",
            "\n",
            "    accuracy                           0.81       120\n",
            "   macro avg       0.81      0.81      0.81       120\n",
            "weighted avg       0.81      0.81      0.81       120\n",
            "\n",
            "\n",
            "âœ“ Classification report saved to: ml_classification_results/classification_report_SVM_(RBF).xlsx\n",
            "âœ“ Scaler saved to: ml_classification_results/scaler.pkl\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ ML BENCHMARKING PIPELINE COMPLETE!\n",
            "================================================================================\n",
            "ðŸ“ All results saved to: ml_classification_results/\n",
            "ðŸ† Best Model: SVM (RBF)\n",
            "ðŸ“Š F1-Score: 0.8160\n",
            "ðŸ“Š Accuracy: 0.8083\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}